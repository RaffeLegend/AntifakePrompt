{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import lavis\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "import random \n",
    "random.seed(43)\n",
    "\n",
    "EXT = ['.jpg', '.jpeg', '.png']\n",
    "\n",
    "class TextInvDataset(Dataset):\n",
    "    def __init__(self, roots, labels, vis_processors=None, txt_processors=None):\n",
    "        \n",
    "        self.path_and_labels = {\n",
    "            'img_path': [],\n",
    "            'label': []\n",
    "        }\n",
    "        \n",
    "        assert len(roots) == len(labels), \"Please assign a label for each image root.\"\n",
    "        \n",
    "        for root, label in zip(roots, labels):\n",
    "            n_sample = 0\n",
    "            for r, dirs, files in os.walk(root):\n",
    "                for file in files:\n",
    "                    if os.path.splitext(file)[-1] in EXT:\n",
    "                        self.path_and_labels[\"img_path\"].append(os.path.join(r, file))\n",
    "                        self.path_and_labels[\"label\"].append(label)\n",
    "                        n_sample += 1\n",
    "            print(f'Found {n_sample} images with label \"{label}\".')\n",
    "        \n",
    "        self.path_and_labels = pd.DataFrame.from_dict(self.path_and_labels)\n",
    "        self.path_and_labels.set_index(\"img_path\", inplace=True)\n",
    "        \n",
    "        self.vis_processors = vis_processors\n",
    "        self.txt_processors = txt_processors\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(list(self.path_and_labels.index))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image_path = list(self.path_and_labels.index)[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.vis_processors:\n",
    "            image = self.vis_processors(image)\n",
    "        \n",
    "        label = self.path_and_labels.loc[image_path, \"label\"]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# This is for query lots of images\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "class InstructBLIP():\n",
    "    def __init__(self, name=\"blip2_vicuna_instruct_textinv\", model_type=\"vicuna7b\", is_eval=True, device=\"cpu\") -> None:\n",
    "        print(f'Loading model...')\n",
    "        #self.model, self.vis_processors, self.txt_processors = load_model_and_preprocess(name, model_type, is_eval, device)\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # QA\n",
    "        self.question = \"\"\n",
    "        \n",
    "        # results\n",
    "        self.acc = None\n",
    "        self.confusion_mat = None\n",
    "        \n",
    "        self.acc_3class = None\n",
    "        self.confusion_mat_3class = None\n",
    "        \n",
    "        self.com_acc = None\n",
    "        self.com_confusion_mat = None\n",
    "        self.uncom_acc = None\n",
    "        self.uncom_confusion_mat = None\n",
    "\n",
    "    def LoadModels(self, model, vis_processors, txt_processors, device):\n",
    "        self.model = model\n",
    "        self.vis_processors = vis_processors\n",
    "        self.txt_processors = txt_processors\n",
    "        self.device = device\n",
    "        \n",
    "    def LoadData(self, roots, labels):\n",
    "        self.roots = [roots] if isinstance(roots, str) else roots\n",
    "        self.text_labels = [labels] if isinstance(labels, str) else labels\n",
    "        self.dataset = TextInvDataset(self.roots, self.text_labels, vis_processors=self.vis_processors[\"eval\"])\n",
    "        self.dataloader = DataLoader(dataset=self.dataset, batch_size=8, shuffle=False, num_workers=8)    \n",
    "    \n",
    "    def QueryImgs_batch(self, question, true_string=\"yes\", logPath='log.txt'):\n",
    "        self.labels = []\n",
    "        self.label_3class = []\n",
    "        self.ans_list = []\n",
    "        self.question = question\n",
    "        \n",
    "        for image, label in tqdm(self.dataloader):\n",
    "            \n",
    "            image = image.to(self.device)\n",
    "            \n",
    "            questions = [self.question] * image.shape[0]\n",
    "            \n",
    "            # samples = {\"image\": image, \"text_input\": questions}\n",
    "            # ans = self.model.predict_answers(samples=samples, inference_method=\"generate\", answer_list=[\"yes\", \"no\"])\n",
    "            # pred_label = [0 if a == true_string else 1 for a in ans]\n",
    "            \n",
    "            samples = {\"image\": image, \"prompt\": questions}\n",
    "            candidates = [\"yes\", \"no\"]\n",
    "            ans = self.model.predict_class(samples=samples, candidates=candidates)\n",
    "            pred_label = [0 if candidates[list(a).index(0)]==true_string else 1 for a in ans]\n",
    "            self.ans_list += pred_label\n",
    "            \n",
    "            label = [0 if l == true_string else 1 for l in label]\n",
    "            self.labels += label\n",
    "        \n",
    "        self.acc = accuracy_score(self.labels, self.ans_list)\n",
    "        self.confusion_mat = confusion_matrix(self.labels, self.ans_list, labels=[0,1])\n",
    "        \n",
    "        self.ans_list = np.array(self.ans_list)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.label_3class = np.array(self.label_3class)\n",
    "        \n",
    "        self.PrintResult(detailed=True, logPath=logPath)\n",
    "        \n",
    "        return self.acc, self.confusion_mat, self.ans_list, self.labels, self.label_3class\n",
    "    \n",
    "    def Query(self, image, question):\n",
    "        image = self.vis_processors[\"eval\"](image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        samples = {\"image\": image, \"prompt\": question}\n",
    "        candidates = [\"yes\", \"no\"]\n",
    "        ans = self.model.predict_class(samples=samples, candidates=candidates)\n",
    "        pred_label = [\"True\" if candidates[list(a).index(0)]==\"yes\" else \"Fake\" for a in ans]\n",
    "        return pred_label\n",
    "\n",
    "    def PrintResult(self, detailed=False, acc=None, confusion_mat=None, ans_list=None, labels=None, logPath=None):\n",
    "        \n",
    "        if acc:\n",
    "            self.acc = acc\n",
    "        if confusion_mat:\n",
    "            self.confusion_mat = confusion_mat\n",
    "        if ans_list:\n",
    "            self.ans_list = ans_list\n",
    "        if labels:\n",
    "            self.labels = labels\n",
    "        \n",
    "        if logPath:\n",
    "            logfile = open(logPath, 'a')\n",
    "        \n",
    "        if detailed:\n",
    "            \n",
    "            print(f'[TIME]      : {time.ctime()}', file=logfile)\n",
    "            print(f'[Finetuned] : {self.model.finetuned}', file=logfile)\n",
    "            print(f'[Img roots] : {self.roots}', file=logfile)\n",
    "            print(f'[Labels]    : {self.text_labels}', file=logfile)\n",
    "            print(f'[Question]  : {self.question}\\n', file=logfile)\n",
    "            \n",
    "            print(f'=== Overall ===', file=logfile)\n",
    "            print(f'Acc: {self.acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "            \n",
    "            if 0 in self.labels:\n",
    "                real_ans_list = self.ans_list[self.labels==0]\n",
    "                real_label = [0] * len(real_ans_list)\n",
    "                self.real_acc = accuracy_score(real_label, real_ans_list)\n",
    "                self.real_confusion_mat = confusion_matrix(real_label, real_ans_list, labels=[0,1])\n",
    "                print(f'=== Real images ===', file=logfile)\n",
    "                print(f'Acc: {self.real_acc*100:.2f}%', file=logfile)\n",
    "                self.PrintConfusion(self.real_confusion_mat, logfile=logfile)\n",
    "                print('\\n', file=logfile)\n",
    "            else:\n",
    "                print(f'=== No real images ===\\n', file=logfile)\n",
    "            \n",
    "            \n",
    "            if 1 in self.labels:\n",
    "                fake_ans_list = self.ans_list[self.labels==1]\n",
    "                fake_label = [1] * len(fake_ans_list)\n",
    "                self.com_acc = accuracy_score(fake_label, fake_ans_list)\n",
    "                self.com_confusion_mat = confusion_matrix(fake_label, fake_ans_list, labels=[0,1])\n",
    "                print(f'=== Fake images ===', file=logfile)\n",
    "                print(f'Acc: {self.com_acc*100:.2f}%', file=logfile)\n",
    "                self.PrintConfusion(self.com_confusion_mat, logfile=logfile)\n",
    "                print('\\n', file=logfile)\n",
    "            else:\n",
    "                print(f'=== No fake images ===\\n', file=logfile)\n",
    "        else:\n",
    "            print(f'Question: {self.question}\\n', file=logfile)\n",
    "            print(f'Acc: {self.acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "        \n",
    "        logfile.close()\n",
    "    \n",
    "    def PrintConfusion(self, mat, logfile):\n",
    "        padding = ' '\n",
    "        print(f'        | Pred real | Pred fake |', file=logfile)\n",
    "        print(f'GT real | {mat[0, 0]:{padding}<{10}}| {mat[0, 1]:{padding}<{11}}|', file=logfile)\n",
    "        print(f'GT fake | {mat[1, 0]:{padding}<{10}}| {mat[1, 1]:{padding}<{11}}|', file=logfile)\n",
    "        \n",
    "    def MultipleAns(self, ans1, ans2):\n",
    "    \n",
    "        # Q1: Is this photo common in real world?\n",
    "        # Q2: Is this photo generated by a model?\n",
    "        \n",
    "        final_ans = []\n",
    "        for ans in zip(ans1, ans2):\n",
    "            if ans[0] == 0 and ans[1] == 0:\n",
    "                final_ans.append(0)\n",
    "            else:\n",
    "                final_ans.append(1)\n",
    "        \n",
    "        acc = accuracy_score(self.labels, final_ans)\n",
    "        confusion_mat = confusion_matrix(self.labels, final_ans)\n",
    "        print(f'Accuracy: {acc*100:.2f}%')\n",
    "        self.PrintConfusion(confusion_mat)\n",
    "        \n",
    "        self.ans_list = final_ans\n",
    "        self.acc = acc\n",
    "        self.confusion_mat = confusion_mat\n",
    "        \n",
    "        return acc, confusion_mat, final_ans\n",
    "    \n",
    "def print_combine_result(pretrained_ans, finetuned_ans, label, logPath):\n",
    "    \n",
    "    logfile = open(logPath, 'a')\n",
    "    \n",
    "    def _print_confusion(mat, logfile):\n",
    "        padding = ' '\n",
    "        print(f'        | Pred real | Pred fake |', file=logfile)\n",
    "        print(f'GT real | {mat[0, 0]:{padding}<{10}}| {mat[0, 1]:{padding}<{11}}|', file=logfile)\n",
    "        print(f'GT fake | {mat[1, 0]:{padding}<{10}}| {mat[1, 1]:{padding}<{11}}|', file=logfile)\n",
    "    \n",
    "    comb_ans = np.ceil((pretrained_ans + finetuned_ans)/2).astype(np.int64)\n",
    "    \n",
    "    comb_acc = accuracy_score(label, comb_ans)\n",
    "    comb_confusion_mat = confusion_matrix(label, comb_ans, labels=[0,1])\n",
    "    \n",
    "    print(f'=== Overall (Comb) ===', file=logfile)\n",
    "    print(f'Acc: {comb_acc*100:.2f}%', file=logfile)\n",
    "    _print_confusion(comb_confusion_mat, logfile=logfile)\n",
    "    print('\\n', file=logfile)\n",
    "    \n",
    "    real_ans_list = comb_ans[label==0]\n",
    "    real_label = [0] * len(real_ans_list)\n",
    "    real_acc = accuracy_score(real_label, real_ans_list)\n",
    "    real_confusion_mat = confusion_matrix(real_label, real_ans_list, labels=[0,1])\n",
    "    print(f'=== Real images (Comb) ===', file=logfile)\n",
    "    print(f'Acc: {real_acc*100:.2f}%', file=logfile)\n",
    "    _print_confusion(real_confusion_mat, logfile=logfile)\n",
    "    print('\\n', file=logfile)\n",
    "    \n",
    "    \n",
    "    com_ans_list = comb_ans[label==1]\n",
    "    com_label = [1] * len(com_ans_list)\n",
    "    com_acc = accuracy_score(com_label, com_ans_list)\n",
    "    com_confusion_mat = confusion_matrix(com_label, com_ans_list, labels=[0,1])\n",
    "    print(f'=== Common fake images (Comb) ===', file=logfile)\n",
    "    print(f'Acc: {com_acc*100:.2f}%', file=logfile)\n",
    "    _print_confusion(com_confusion_mat, logfile=logfile)\n",
    "    print('\\n', file=logfile)\n",
    "    \n",
    "    return comb_acc, comb_confusion_mat, comb_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer OK!\n",
      "visual encoder OK!\n",
      "Q-former OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM OK!\n",
      "Added 1 tokens to Qformer tokenizer.\n",
      "Added 1 tokens to llm tokenizer.\n",
      "Missing keys ['visual_encoder.cls_token', 'visual_encoder.pos_embed', 'visual_encoder.patch_embed.proj.weight', 'visual_encoder.patch_embed.proj.bias', 'visual_encoder.blocks.0.norm1.weight', 'visual_encoder.blocks.0.norm1.bias', 'visual_encoder.blocks.0.attn.q_bias', 'visual_encoder.blocks.0.attn.v_bias', 'visual_encoder.blocks.0.attn.qkv.weight', 'visual_encoder.blocks.0.attn.proj.weight', 'visual_encoder.blocks.0.attn.proj.bias', 'visual_encoder.blocks.0.norm2.weight', 'visual_encoder.blocks.0.norm2.bias', 'visual_encoder.blocks.0.mlp.fc1.weight', 'visual_encoder.blocks.0.mlp.fc1.bias', 'visual_encoder.blocks.0.mlp.fc2.weight', 'visual_encoder.blocks.0.mlp.fc2.bias', 'visual_encoder.blocks.1.norm1.weight', 'visual_encoder.blocks.1.norm1.bias', 'visual_encoder.blocks.1.attn.q_bias', 'visual_encoder.blocks.1.attn.v_bias', 'visual_encoder.blocks.1.attn.qkv.weight', 'visual_encoder.blocks.1.attn.proj.weight', 'visual_encoder.blocks.1.attn.proj.bias', 'visual_encoder.blocks.1.norm2.weight', 'visual_encoder.blocks.1.norm2.bias', 'visual_encoder.blocks.1.mlp.fc1.weight', 'visual_encoder.blocks.1.mlp.fc1.bias', 'visual_encoder.blocks.1.mlp.fc2.weight', 'visual_encoder.blocks.1.mlp.fc2.bias', 'visual_encoder.blocks.2.norm1.weight', 'visual_encoder.blocks.2.norm1.bias', 'visual_encoder.blocks.2.attn.q_bias', 'visual_encoder.blocks.2.attn.v_bias', 'visual_encoder.blocks.2.attn.qkv.weight', 'visual_encoder.blocks.2.attn.proj.weight', 'visual_encoder.blocks.2.attn.proj.bias', 'visual_encoder.blocks.2.norm2.weight', 'visual_encoder.blocks.2.norm2.bias', 'visual_encoder.blocks.2.mlp.fc1.weight', 'visual_encoder.blocks.2.mlp.fc1.bias', 'visual_encoder.blocks.2.mlp.fc2.weight', 'visual_encoder.blocks.2.mlp.fc2.bias', 'visual_encoder.blocks.3.norm1.weight', 'visual_encoder.blocks.3.norm1.bias', 'visual_encoder.blocks.3.attn.q_bias', 'visual_encoder.blocks.3.attn.v_bias', 'visual_encoder.blocks.3.attn.qkv.weight', 'visual_encoder.blocks.3.attn.proj.weight', 'visual_encoder.blocks.3.attn.proj.bias', 'visual_encoder.blocks.3.norm2.weight', 'visual_encoder.blocks.3.norm2.bias', 'visual_encoder.blocks.3.mlp.fc1.weight', 'visual_encoder.blocks.3.mlp.fc1.bias', 'visual_encoder.blocks.3.mlp.fc2.weight', 'visual_encoder.blocks.3.mlp.fc2.bias', 'visual_encoder.blocks.4.norm1.weight', 'visual_encoder.blocks.4.norm1.bias', 'visual_encoder.blocks.4.attn.q_bias', 'visual_encoder.blocks.4.attn.v_bias', 'visual_encoder.blocks.4.attn.qkv.weight', 'visual_encoder.blocks.4.attn.proj.weight', 'visual_encoder.blocks.4.attn.proj.bias', 'visual_encoder.blocks.4.norm2.weight', 'visual_encoder.blocks.4.norm2.bias', 'visual_encoder.blocks.4.mlp.fc1.weight', 'visual_encoder.blocks.4.mlp.fc1.bias', 'visual_encoder.blocks.4.mlp.fc2.weight', 'visual_encoder.blocks.4.mlp.fc2.bias', 'visual_encoder.blocks.5.norm1.weight', 'visual_encoder.blocks.5.norm1.bias', 'visual_encoder.blocks.5.attn.q_bias', 'visual_encoder.blocks.5.attn.v_bias', 'visual_encoder.blocks.5.attn.qkv.weight', 'visual_encoder.blocks.5.attn.proj.weight', 'visual_encoder.blocks.5.attn.proj.bias', 'visual_encoder.blocks.5.norm2.weight', 'visual_encoder.blocks.5.norm2.bias', 'visual_encoder.blocks.5.mlp.fc1.weight', 'visual_encoder.blocks.5.mlp.fc1.bias', 'visual_encoder.blocks.5.mlp.fc2.weight', 'visual_encoder.blocks.5.mlp.fc2.bias', 'visual_encoder.blocks.6.norm1.weight', 'visual_encoder.blocks.6.norm1.bias', 'visual_encoder.blocks.6.attn.q_bias', 'visual_encoder.blocks.6.attn.v_bias', 'visual_encoder.blocks.6.attn.qkv.weight', 'visual_encoder.blocks.6.attn.proj.weight', 'visual_encoder.blocks.6.attn.proj.bias', 'visual_encoder.blocks.6.norm2.weight', 'visual_encoder.blocks.6.norm2.bias', 'visual_encoder.blocks.6.mlp.fc1.weight', 'visual_encoder.blocks.6.mlp.fc1.bias', 'visual_encoder.blocks.6.mlp.fc2.weight', 'visual_encoder.blocks.6.mlp.fc2.bias', 'visual_encoder.blocks.7.norm1.weight', 'visual_encoder.blocks.7.norm1.bias', 'visual_encoder.blocks.7.attn.q_bias', 'visual_encoder.blocks.7.attn.v_bias', 'visual_encoder.blocks.7.attn.qkv.weight', 'visual_encoder.blocks.7.attn.proj.weight', 'visual_encoder.blocks.7.attn.proj.bias', 'visual_encoder.blocks.7.norm2.weight', 'visual_encoder.blocks.7.norm2.bias', 'visual_encoder.blocks.7.mlp.fc1.weight', 'visual_encoder.blocks.7.mlp.fc1.bias', 'visual_encoder.blocks.7.mlp.fc2.weight', 'visual_encoder.blocks.7.mlp.fc2.bias', 'visual_encoder.blocks.8.norm1.weight', 'visual_encoder.blocks.8.norm1.bias', 'visual_encoder.blocks.8.attn.q_bias', 'visual_encoder.blocks.8.attn.v_bias', 'visual_encoder.blocks.8.attn.qkv.weight', 'visual_encoder.blocks.8.attn.proj.weight', 'visual_encoder.blocks.8.attn.proj.bias', 'visual_encoder.blocks.8.norm2.weight', 'visual_encoder.blocks.8.norm2.bias', 'visual_encoder.blocks.8.mlp.fc1.weight', 'visual_encoder.blocks.8.mlp.fc1.bias', 'visual_encoder.blocks.8.mlp.fc2.weight', 'visual_encoder.blocks.8.mlp.fc2.bias', 'visual_encoder.blocks.9.norm1.weight', 'visual_encoder.blocks.9.norm1.bias', 'visual_encoder.blocks.9.attn.q_bias', 'visual_encoder.blocks.9.attn.v_bias', 'visual_encoder.blocks.9.attn.qkv.weight', 'visual_encoder.blocks.9.attn.proj.weight', 'visual_encoder.blocks.9.attn.proj.bias', 'visual_encoder.blocks.9.norm2.weight', 'visual_encoder.blocks.9.norm2.bias', 'visual_encoder.blocks.9.mlp.fc1.weight', 'visual_encoder.blocks.9.mlp.fc1.bias', 'visual_encoder.blocks.9.mlp.fc2.weight', 'visual_encoder.blocks.9.mlp.fc2.bias', 'visual_encoder.blocks.10.norm1.weight', 'visual_encoder.blocks.10.norm1.bias', 'visual_encoder.blocks.10.attn.q_bias', 'visual_encoder.blocks.10.attn.v_bias', 'visual_encoder.blocks.10.attn.qkv.weight', 'visual_encoder.blocks.10.attn.proj.weight', 'visual_encoder.blocks.10.attn.proj.bias', 'visual_encoder.blocks.10.norm2.weight', 'visual_encoder.blocks.10.norm2.bias', 'visual_encoder.blocks.10.mlp.fc1.weight', 'visual_encoder.blocks.10.mlp.fc1.bias', 'visual_encoder.blocks.10.mlp.fc2.weight', 'visual_encoder.blocks.10.mlp.fc2.bias', 'visual_encoder.blocks.11.norm1.weight', 'visual_encoder.blocks.11.norm1.bias', 'visual_encoder.blocks.11.attn.q_bias', 'visual_encoder.blocks.11.attn.v_bias', 'visual_encoder.blocks.11.attn.qkv.weight', 'visual_encoder.blocks.11.attn.proj.weight', 'visual_encoder.blocks.11.attn.proj.bias', 'visual_encoder.blocks.11.norm2.weight', 'visual_encoder.blocks.11.norm2.bias', 'visual_encoder.blocks.11.mlp.fc1.weight', 'visual_encoder.blocks.11.mlp.fc1.bias', 'visual_encoder.blocks.11.mlp.fc2.weight', 'visual_encoder.blocks.11.mlp.fc2.bias', 'visual_encoder.blocks.12.norm1.weight', 'visual_encoder.blocks.12.norm1.bias', 'visual_encoder.blocks.12.attn.q_bias', 'visual_encoder.blocks.12.attn.v_bias', 'visual_encoder.blocks.12.attn.qkv.weight', 'visual_encoder.blocks.12.attn.proj.weight', 'visual_encoder.blocks.12.attn.proj.bias', 'visual_encoder.blocks.12.norm2.weight', 'visual_encoder.blocks.12.norm2.bias', 'visual_encoder.blocks.12.mlp.fc1.weight', 'visual_encoder.blocks.12.mlp.fc1.bias', 'visual_encoder.blocks.12.mlp.fc2.weight', 'visual_encoder.blocks.12.mlp.fc2.bias', 'visual_encoder.blocks.13.norm1.weight', 'visual_encoder.blocks.13.norm1.bias', 'visual_encoder.blocks.13.attn.q_bias', 'visual_encoder.blocks.13.attn.v_bias', 'visual_encoder.blocks.13.attn.qkv.weight', 'visual_encoder.blocks.13.attn.proj.weight', 'visual_encoder.blocks.13.attn.proj.bias', 'visual_encoder.blocks.13.norm2.weight', 'visual_encoder.blocks.13.norm2.bias', 'visual_encoder.blocks.13.mlp.fc1.weight', 'visual_encoder.blocks.13.mlp.fc1.bias', 'visual_encoder.blocks.13.mlp.fc2.weight', 'visual_encoder.blocks.13.mlp.fc2.bias', 'visual_encoder.blocks.14.norm1.weight', 'visual_encoder.blocks.14.norm1.bias', 'visual_encoder.blocks.14.attn.q_bias', 'visual_encoder.blocks.14.attn.v_bias', 'visual_encoder.blocks.14.attn.qkv.weight', 'visual_encoder.blocks.14.attn.proj.weight', 'visual_encoder.blocks.14.attn.proj.bias', 'visual_encoder.blocks.14.norm2.weight', 'visual_encoder.blocks.14.norm2.bias', 'visual_encoder.blocks.14.mlp.fc1.weight', 'visual_encoder.blocks.14.mlp.fc1.bias', 'visual_encoder.blocks.14.mlp.fc2.weight', 'visual_encoder.blocks.14.mlp.fc2.bias', 'visual_encoder.blocks.15.norm1.weight', 'visual_encoder.blocks.15.norm1.bias', 'visual_encoder.blocks.15.attn.q_bias', 'visual_encoder.blocks.15.attn.v_bias', 'visual_encoder.blocks.15.attn.qkv.weight', 'visual_encoder.blocks.15.attn.proj.weight', 'visual_encoder.blocks.15.attn.proj.bias', 'visual_encoder.blocks.15.norm2.weight', 'visual_encoder.blocks.15.norm2.bias', 'visual_encoder.blocks.15.mlp.fc1.weight', 'visual_encoder.blocks.15.mlp.fc1.bias', 'visual_encoder.blocks.15.mlp.fc2.weight', 'visual_encoder.blocks.15.mlp.fc2.bias', 'visual_encoder.blocks.16.norm1.weight', 'visual_encoder.blocks.16.norm1.bias', 'visual_encoder.blocks.16.attn.q_bias', 'visual_encoder.blocks.16.attn.v_bias', 'visual_encoder.blocks.16.attn.qkv.weight', 'visual_encoder.blocks.16.attn.proj.weight', 'visual_encoder.blocks.16.attn.proj.bias', 'visual_encoder.blocks.16.norm2.weight', 'visual_encoder.blocks.16.norm2.bias', 'visual_encoder.blocks.16.mlp.fc1.weight', 'visual_encoder.blocks.16.mlp.fc1.bias', 'visual_encoder.blocks.16.mlp.fc2.weight', 'visual_encoder.blocks.16.mlp.fc2.bias', 'visual_encoder.blocks.17.norm1.weight', 'visual_encoder.blocks.17.norm1.bias', 'visual_encoder.blocks.17.attn.q_bias', 'visual_encoder.blocks.17.attn.v_bias', 'visual_encoder.blocks.17.attn.qkv.weight', 'visual_encoder.blocks.17.attn.proj.weight', 'visual_encoder.blocks.17.attn.proj.bias', 'visual_encoder.blocks.17.norm2.weight', 'visual_encoder.blocks.17.norm2.bias', 'visual_encoder.blocks.17.mlp.fc1.weight', 'visual_encoder.blocks.17.mlp.fc1.bias', 'visual_encoder.blocks.17.mlp.fc2.weight', 'visual_encoder.blocks.17.mlp.fc2.bias', 'visual_encoder.blocks.18.norm1.weight', 'visual_encoder.blocks.18.norm1.bias', 'visual_encoder.blocks.18.attn.q_bias', 'visual_encoder.blocks.18.attn.v_bias', 'visual_encoder.blocks.18.attn.qkv.weight', 'visual_encoder.blocks.18.attn.proj.weight', 'visual_encoder.blocks.18.attn.proj.bias', 'visual_encoder.blocks.18.norm2.weight', 'visual_encoder.blocks.18.norm2.bias', 'visual_encoder.blocks.18.mlp.fc1.weight', 'visual_encoder.blocks.18.mlp.fc1.bias', 'visual_encoder.blocks.18.mlp.fc2.weight', 'visual_encoder.blocks.18.mlp.fc2.bias', 'visual_encoder.blocks.19.norm1.weight', 'visual_encoder.blocks.19.norm1.bias', 'visual_encoder.blocks.19.attn.q_bias', 'visual_encoder.blocks.19.attn.v_bias', 'visual_encoder.blocks.19.attn.qkv.weight', 'visual_encoder.blocks.19.attn.proj.weight', 'visual_encoder.blocks.19.attn.proj.bias', 'visual_encoder.blocks.19.norm2.weight', 'visual_encoder.blocks.19.norm2.bias', 'visual_encoder.blocks.19.mlp.fc1.weight', 'visual_encoder.blocks.19.mlp.fc1.bias', 'visual_encoder.blocks.19.mlp.fc2.weight', 'visual_encoder.blocks.19.mlp.fc2.bias', 'visual_encoder.blocks.20.norm1.weight', 'visual_encoder.blocks.20.norm1.bias', 'visual_encoder.blocks.20.attn.q_bias', 'visual_encoder.blocks.20.attn.v_bias', 'visual_encoder.blocks.20.attn.qkv.weight', 'visual_encoder.blocks.20.attn.proj.weight', 'visual_encoder.blocks.20.attn.proj.bias', 'visual_encoder.blocks.20.norm2.weight', 'visual_encoder.blocks.20.norm2.bias', 'visual_encoder.blocks.20.mlp.fc1.weight', 'visual_encoder.blocks.20.mlp.fc1.bias', 'visual_encoder.blocks.20.mlp.fc2.weight', 'visual_encoder.blocks.20.mlp.fc2.bias', 'visual_encoder.blocks.21.norm1.weight', 'visual_encoder.blocks.21.norm1.bias', 'visual_encoder.blocks.21.attn.q_bias', 'visual_encoder.blocks.21.attn.v_bias', 'visual_encoder.blocks.21.attn.qkv.weight', 'visual_encoder.blocks.21.attn.proj.weight', 'visual_encoder.blocks.21.attn.proj.bias', 'visual_encoder.blocks.21.norm2.weight', 'visual_encoder.blocks.21.norm2.bias', 'visual_encoder.blocks.21.mlp.fc1.weight', 'visual_encoder.blocks.21.mlp.fc1.bias', 'visual_encoder.blocks.21.mlp.fc2.weight', 'visual_encoder.blocks.21.mlp.fc2.bias', 'visual_encoder.blocks.22.norm1.weight', 'visual_encoder.blocks.22.norm1.bias', 'visual_encoder.blocks.22.attn.q_bias', 'visual_encoder.blocks.22.attn.v_bias', 'visual_encoder.blocks.22.attn.qkv.weight', 'visual_encoder.blocks.22.attn.proj.weight', 'visual_encoder.blocks.22.attn.proj.bias', 'visual_encoder.blocks.22.norm2.weight', 'visual_encoder.blocks.22.norm2.bias', 'visual_encoder.blocks.22.mlp.fc1.weight', 'visual_encoder.blocks.22.mlp.fc1.bias', 'visual_encoder.blocks.22.mlp.fc2.weight', 'visual_encoder.blocks.22.mlp.fc2.bias', 'visual_encoder.blocks.23.norm1.weight', 'visual_encoder.blocks.23.norm1.bias', 'visual_encoder.blocks.23.attn.q_bias', 'visual_encoder.blocks.23.attn.v_bias', 'visual_encoder.blocks.23.attn.qkv.weight', 'visual_encoder.blocks.23.attn.proj.weight', 'visual_encoder.blocks.23.attn.proj.bias', 'visual_encoder.blocks.23.norm2.weight', 'visual_encoder.blocks.23.norm2.bias', 'visual_encoder.blocks.23.mlp.fc1.weight', 'visual_encoder.blocks.23.mlp.fc1.bias', 'visual_encoder.blocks.23.mlp.fc2.weight', 'visual_encoder.blocks.23.mlp.fc2.bias', 'visual_encoder.blocks.24.norm1.weight', 'visual_encoder.blocks.24.norm1.bias', 'visual_encoder.blocks.24.attn.q_bias', 'visual_encoder.blocks.24.attn.v_bias', 'visual_encoder.blocks.24.attn.qkv.weight', 'visual_encoder.blocks.24.attn.proj.weight', 'visual_encoder.blocks.24.attn.proj.bias', 'visual_encoder.blocks.24.norm2.weight', 'visual_encoder.blocks.24.norm2.bias', 'visual_encoder.blocks.24.mlp.fc1.weight', 'visual_encoder.blocks.24.mlp.fc1.bias', 'visual_encoder.blocks.24.mlp.fc2.weight', 'visual_encoder.blocks.24.mlp.fc2.bias', 'visual_encoder.blocks.25.norm1.weight', 'visual_encoder.blocks.25.norm1.bias', 'visual_encoder.blocks.25.attn.q_bias', 'visual_encoder.blocks.25.attn.v_bias', 'visual_encoder.blocks.25.attn.qkv.weight', 'visual_encoder.blocks.25.attn.proj.weight', 'visual_encoder.blocks.25.attn.proj.bias', 'visual_encoder.blocks.25.norm2.weight', 'visual_encoder.blocks.25.norm2.bias', 'visual_encoder.blocks.25.mlp.fc1.weight', 'visual_encoder.blocks.25.mlp.fc1.bias', 'visual_encoder.blocks.25.mlp.fc2.weight', 'visual_encoder.blocks.25.mlp.fc2.bias', 'visual_encoder.blocks.26.norm1.weight', 'visual_encoder.blocks.26.norm1.bias', 'visual_encoder.blocks.26.attn.q_bias', 'visual_encoder.blocks.26.attn.v_bias', 'visual_encoder.blocks.26.attn.qkv.weight', 'visual_encoder.blocks.26.attn.proj.weight', 'visual_encoder.blocks.26.attn.proj.bias', 'visual_encoder.blocks.26.norm2.weight', 'visual_encoder.blocks.26.norm2.bias', 'visual_encoder.blocks.26.mlp.fc1.weight', 'visual_encoder.blocks.26.mlp.fc1.bias', 'visual_encoder.blocks.26.mlp.fc2.weight', 'visual_encoder.blocks.26.mlp.fc2.bias', 'visual_encoder.blocks.27.norm1.weight', 'visual_encoder.blocks.27.norm1.bias', 'visual_encoder.blocks.27.attn.q_bias', 'visual_encoder.blocks.27.attn.v_bias', 'visual_encoder.blocks.27.attn.qkv.weight', 'visual_encoder.blocks.27.attn.proj.weight', 'visual_encoder.blocks.27.attn.proj.bias', 'visual_encoder.blocks.27.norm2.weight', 'visual_encoder.blocks.27.norm2.bias', 'visual_encoder.blocks.27.mlp.fc1.weight', 'visual_encoder.blocks.27.mlp.fc1.bias', 'visual_encoder.blocks.27.mlp.fc2.weight', 'visual_encoder.blocks.27.mlp.fc2.bias', 'visual_encoder.blocks.28.norm1.weight', 'visual_encoder.blocks.28.norm1.bias', 'visual_encoder.blocks.28.attn.q_bias', 'visual_encoder.blocks.28.attn.v_bias', 'visual_encoder.blocks.28.attn.qkv.weight', 'visual_encoder.blocks.28.attn.proj.weight', 'visual_encoder.blocks.28.attn.proj.bias', 'visual_encoder.blocks.28.norm2.weight', 'visual_encoder.blocks.28.norm2.bias', 'visual_encoder.blocks.28.mlp.fc1.weight', 'visual_encoder.blocks.28.mlp.fc1.bias', 'visual_encoder.blocks.28.mlp.fc2.weight', 'visual_encoder.blocks.28.mlp.fc2.bias', 'visual_encoder.blocks.29.norm1.weight', 'visual_encoder.blocks.29.norm1.bias', 'visual_encoder.blocks.29.attn.q_bias', 'visual_encoder.blocks.29.attn.v_bias', 'visual_encoder.blocks.29.attn.qkv.weight', 'visual_encoder.blocks.29.attn.proj.weight', 'visual_encoder.blocks.29.attn.proj.bias', 'visual_encoder.blocks.29.norm2.weight', 'visual_encoder.blocks.29.norm2.bias', 'visual_encoder.blocks.29.mlp.fc1.weight', 'visual_encoder.blocks.29.mlp.fc1.bias', 'visual_encoder.blocks.29.mlp.fc2.weight', 'visual_encoder.blocks.29.mlp.fc2.bias', 'visual_encoder.blocks.30.norm1.weight', 'visual_encoder.blocks.30.norm1.bias', 'visual_encoder.blocks.30.attn.q_bias', 'visual_encoder.blocks.30.attn.v_bias', 'visual_encoder.blocks.30.attn.qkv.weight', 'visual_encoder.blocks.30.attn.proj.weight', 'visual_encoder.blocks.30.attn.proj.bias', 'visual_encoder.blocks.30.norm2.weight', 'visual_encoder.blocks.30.norm2.bias', 'visual_encoder.blocks.30.mlp.fc1.weight', 'visual_encoder.blocks.30.mlp.fc1.bias', 'visual_encoder.blocks.30.mlp.fc2.weight', 'visual_encoder.blocks.30.mlp.fc2.bias', 'visual_encoder.blocks.31.norm1.weight', 'visual_encoder.blocks.31.norm1.bias', 'visual_encoder.blocks.31.attn.q_bias', 'visual_encoder.blocks.31.attn.v_bias', 'visual_encoder.blocks.31.attn.qkv.weight', 'visual_encoder.blocks.31.attn.proj.weight', 'visual_encoder.blocks.31.attn.proj.bias', 'visual_encoder.blocks.31.norm2.weight', 'visual_encoder.blocks.31.norm2.bias', 'visual_encoder.blocks.31.mlp.fc1.weight', 'visual_encoder.blocks.31.mlp.fc1.bias', 'visual_encoder.blocks.31.mlp.fc2.weight', 'visual_encoder.blocks.31.mlp.fc2.bias', 'visual_encoder.blocks.32.norm1.weight', 'visual_encoder.blocks.32.norm1.bias', 'visual_encoder.blocks.32.attn.q_bias', 'visual_encoder.blocks.32.attn.v_bias', 'visual_encoder.blocks.32.attn.qkv.weight', 'visual_encoder.blocks.32.attn.proj.weight', 'visual_encoder.blocks.32.attn.proj.bias', 'visual_encoder.blocks.32.norm2.weight', 'visual_encoder.blocks.32.norm2.bias', 'visual_encoder.blocks.32.mlp.fc1.weight', 'visual_encoder.blocks.32.mlp.fc1.bias', 'visual_encoder.blocks.32.mlp.fc2.weight', 'visual_encoder.blocks.32.mlp.fc2.bias', 'visual_encoder.blocks.33.norm1.weight', 'visual_encoder.blocks.33.norm1.bias', 'visual_encoder.blocks.33.attn.q_bias', 'visual_encoder.blocks.33.attn.v_bias', 'visual_encoder.blocks.33.attn.qkv.weight', 'visual_encoder.blocks.33.attn.proj.weight', 'visual_encoder.blocks.33.attn.proj.bias', 'visual_encoder.blocks.33.norm2.weight', 'visual_encoder.blocks.33.norm2.bias', 'visual_encoder.blocks.33.mlp.fc1.weight', 'visual_encoder.blocks.33.mlp.fc1.bias', 'visual_encoder.blocks.33.mlp.fc2.weight', 'visual_encoder.blocks.33.mlp.fc2.bias', 'visual_encoder.blocks.34.norm1.weight', 'visual_encoder.blocks.34.norm1.bias', 'visual_encoder.blocks.34.attn.q_bias', 'visual_encoder.blocks.34.attn.v_bias', 'visual_encoder.blocks.34.attn.qkv.weight', 'visual_encoder.blocks.34.attn.proj.weight', 'visual_encoder.blocks.34.attn.proj.bias', 'visual_encoder.blocks.34.norm2.weight', 'visual_encoder.blocks.34.norm2.bias', 'visual_encoder.blocks.34.mlp.fc1.weight', 'visual_encoder.blocks.34.mlp.fc1.bias', 'visual_encoder.blocks.34.mlp.fc2.weight', 'visual_encoder.blocks.34.mlp.fc2.bias', 'visual_encoder.blocks.35.norm1.weight', 'visual_encoder.blocks.35.norm1.bias', 'visual_encoder.blocks.35.attn.q_bias', 'visual_encoder.blocks.35.attn.v_bias', 'visual_encoder.blocks.35.attn.qkv.weight', 'visual_encoder.blocks.35.attn.proj.weight', 'visual_encoder.blocks.35.attn.proj.bias', 'visual_encoder.blocks.35.norm2.weight', 'visual_encoder.blocks.35.norm2.bias', 'visual_encoder.blocks.35.mlp.fc1.weight', 'visual_encoder.blocks.35.mlp.fc1.bias', 'visual_encoder.blocks.35.mlp.fc2.weight', 'visual_encoder.blocks.35.mlp.fc2.bias', 'visual_encoder.blocks.36.norm1.weight', 'visual_encoder.blocks.36.norm1.bias', 'visual_encoder.blocks.36.attn.q_bias', 'visual_encoder.blocks.36.attn.v_bias', 'visual_encoder.blocks.36.attn.qkv.weight', 'visual_encoder.blocks.36.attn.proj.weight', 'visual_encoder.blocks.36.attn.proj.bias', 'visual_encoder.blocks.36.norm2.weight', 'visual_encoder.blocks.36.norm2.bias', 'visual_encoder.blocks.36.mlp.fc1.weight', 'visual_encoder.blocks.36.mlp.fc1.bias', 'visual_encoder.blocks.36.mlp.fc2.weight', 'visual_encoder.blocks.36.mlp.fc2.bias', 'visual_encoder.blocks.37.norm1.weight', 'visual_encoder.blocks.37.norm1.bias', 'visual_encoder.blocks.37.attn.q_bias', 'visual_encoder.blocks.37.attn.v_bias', 'visual_encoder.blocks.37.attn.qkv.weight', 'visual_encoder.blocks.37.attn.proj.weight', 'visual_encoder.blocks.37.attn.proj.bias', 'visual_encoder.blocks.37.norm2.weight', 'visual_encoder.blocks.37.norm2.bias', 'visual_encoder.blocks.37.mlp.fc1.weight', 'visual_encoder.blocks.37.mlp.fc1.bias', 'visual_encoder.blocks.37.mlp.fc2.weight', 'visual_encoder.blocks.37.mlp.fc2.bias', 'visual_encoder.blocks.38.norm1.weight', 'visual_encoder.blocks.38.norm1.bias', 'visual_encoder.blocks.38.attn.q_bias', 'visual_encoder.blocks.38.attn.v_bias', 'visual_encoder.blocks.38.attn.qkv.weight', 'visual_encoder.blocks.38.attn.proj.weight', 'visual_encoder.blocks.38.attn.proj.bias', 'visual_encoder.blocks.38.norm2.weight', 'visual_encoder.blocks.38.norm2.bias', 'visual_encoder.blocks.38.mlp.fc1.weight', 'visual_encoder.blocks.38.mlp.fc1.bias', 'visual_encoder.blocks.38.mlp.fc2.weight', 'visual_encoder.blocks.38.mlp.fc2.bias', 'Qformer.bert.embeddings.train_word_embeddings.weight', 'llm_model.model.embed_tokens.weight', 'llm_model.model.layers.0.self_attn.q_proj.weight', 'llm_model.model.layers.0.self_attn.k_proj.weight', 'llm_model.model.layers.0.self_attn.v_proj.weight', 'llm_model.model.layers.0.self_attn.o_proj.weight', 'llm_model.model.layers.0.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.0.mlp.gate_proj.weight', 'llm_model.model.layers.0.mlp.down_proj.weight', 'llm_model.model.layers.0.mlp.up_proj.weight', 'llm_model.model.layers.0.input_layernorm.weight', 'llm_model.model.layers.0.post_attention_layernorm.weight', 'llm_model.model.layers.1.self_attn.q_proj.weight', 'llm_model.model.layers.1.self_attn.k_proj.weight', 'llm_model.model.layers.1.self_attn.v_proj.weight', 'llm_model.model.layers.1.self_attn.o_proj.weight', 'llm_model.model.layers.1.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.1.mlp.gate_proj.weight', 'llm_model.model.layers.1.mlp.down_proj.weight', 'llm_model.model.layers.1.mlp.up_proj.weight', 'llm_model.model.layers.1.input_layernorm.weight', 'llm_model.model.layers.1.post_attention_layernorm.weight', 'llm_model.model.layers.2.self_attn.q_proj.weight', 'llm_model.model.layers.2.self_attn.k_proj.weight', 'llm_model.model.layers.2.self_attn.v_proj.weight', 'llm_model.model.layers.2.self_attn.o_proj.weight', 'llm_model.model.layers.2.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.2.mlp.gate_proj.weight', 'llm_model.model.layers.2.mlp.down_proj.weight', 'llm_model.model.layers.2.mlp.up_proj.weight', 'llm_model.model.layers.2.input_layernorm.weight', 'llm_model.model.layers.2.post_attention_layernorm.weight', 'llm_model.model.layers.3.self_attn.q_proj.weight', 'llm_model.model.layers.3.self_attn.k_proj.weight', 'llm_model.model.layers.3.self_attn.v_proj.weight', 'llm_model.model.layers.3.self_attn.o_proj.weight', 'llm_model.model.layers.3.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.3.mlp.gate_proj.weight', 'llm_model.model.layers.3.mlp.down_proj.weight', 'llm_model.model.layers.3.mlp.up_proj.weight', 'llm_model.model.layers.3.input_layernorm.weight', 'llm_model.model.layers.3.post_attention_layernorm.weight', 'llm_model.model.layers.4.self_attn.q_proj.weight', 'llm_model.model.layers.4.self_attn.k_proj.weight', 'llm_model.model.layers.4.self_attn.v_proj.weight', 'llm_model.model.layers.4.self_attn.o_proj.weight', 'llm_model.model.layers.4.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.4.mlp.gate_proj.weight', 'llm_model.model.layers.4.mlp.down_proj.weight', 'llm_model.model.layers.4.mlp.up_proj.weight', 'llm_model.model.layers.4.input_layernorm.weight', 'llm_model.model.layers.4.post_attention_layernorm.weight', 'llm_model.model.layers.5.self_attn.q_proj.weight', 'llm_model.model.layers.5.self_attn.k_proj.weight', 'llm_model.model.layers.5.self_attn.v_proj.weight', 'llm_model.model.layers.5.self_attn.o_proj.weight', 'llm_model.model.layers.5.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.5.mlp.gate_proj.weight', 'llm_model.model.layers.5.mlp.down_proj.weight', 'llm_model.model.layers.5.mlp.up_proj.weight', 'llm_model.model.layers.5.input_layernorm.weight', 'llm_model.model.layers.5.post_attention_layernorm.weight', 'llm_model.model.layers.6.self_attn.q_proj.weight', 'llm_model.model.layers.6.self_attn.k_proj.weight', 'llm_model.model.layers.6.self_attn.v_proj.weight', 'llm_model.model.layers.6.self_attn.o_proj.weight', 'llm_model.model.layers.6.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.6.mlp.gate_proj.weight', 'llm_model.model.layers.6.mlp.down_proj.weight', 'llm_model.model.layers.6.mlp.up_proj.weight', 'llm_model.model.layers.6.input_layernorm.weight', 'llm_model.model.layers.6.post_attention_layernorm.weight', 'llm_model.model.layers.7.self_attn.q_proj.weight', 'llm_model.model.layers.7.self_attn.k_proj.weight', 'llm_model.model.layers.7.self_attn.v_proj.weight', 'llm_model.model.layers.7.self_attn.o_proj.weight', 'llm_model.model.layers.7.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.7.mlp.gate_proj.weight', 'llm_model.model.layers.7.mlp.down_proj.weight', 'llm_model.model.layers.7.mlp.up_proj.weight', 'llm_model.model.layers.7.input_layernorm.weight', 'llm_model.model.layers.7.post_attention_layernorm.weight', 'llm_model.model.layers.8.self_attn.q_proj.weight', 'llm_model.model.layers.8.self_attn.k_proj.weight', 'llm_model.model.layers.8.self_attn.v_proj.weight', 'llm_model.model.layers.8.self_attn.o_proj.weight', 'llm_model.model.layers.8.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.8.mlp.gate_proj.weight', 'llm_model.model.layers.8.mlp.down_proj.weight', 'llm_model.model.layers.8.mlp.up_proj.weight', 'llm_model.model.layers.8.input_layernorm.weight', 'llm_model.model.layers.8.post_attention_layernorm.weight', 'llm_model.model.layers.9.self_attn.q_proj.weight', 'llm_model.model.layers.9.self_attn.k_proj.weight', 'llm_model.model.layers.9.self_attn.v_proj.weight', 'llm_model.model.layers.9.self_attn.o_proj.weight', 'llm_model.model.layers.9.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.9.mlp.gate_proj.weight', 'llm_model.model.layers.9.mlp.down_proj.weight', 'llm_model.model.layers.9.mlp.up_proj.weight', 'llm_model.model.layers.9.input_layernorm.weight', 'llm_model.model.layers.9.post_attention_layernorm.weight', 'llm_model.model.layers.10.self_attn.q_proj.weight', 'llm_model.model.layers.10.self_attn.k_proj.weight', 'llm_model.model.layers.10.self_attn.v_proj.weight', 'llm_model.model.layers.10.self_attn.o_proj.weight', 'llm_model.model.layers.10.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.10.mlp.gate_proj.weight', 'llm_model.model.layers.10.mlp.down_proj.weight', 'llm_model.model.layers.10.mlp.up_proj.weight', 'llm_model.model.layers.10.input_layernorm.weight', 'llm_model.model.layers.10.post_attention_layernorm.weight', 'llm_model.model.layers.11.self_attn.q_proj.weight', 'llm_model.model.layers.11.self_attn.k_proj.weight', 'llm_model.model.layers.11.self_attn.v_proj.weight', 'llm_model.model.layers.11.self_attn.o_proj.weight', 'llm_model.model.layers.11.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.11.mlp.gate_proj.weight', 'llm_model.model.layers.11.mlp.down_proj.weight', 'llm_model.model.layers.11.mlp.up_proj.weight', 'llm_model.model.layers.11.input_layernorm.weight', 'llm_model.model.layers.11.post_attention_layernorm.weight', 'llm_model.model.layers.12.self_attn.q_proj.weight', 'llm_model.model.layers.12.self_attn.k_proj.weight', 'llm_model.model.layers.12.self_attn.v_proj.weight', 'llm_model.model.layers.12.self_attn.o_proj.weight', 'llm_model.model.layers.12.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.12.mlp.gate_proj.weight', 'llm_model.model.layers.12.mlp.down_proj.weight', 'llm_model.model.layers.12.mlp.up_proj.weight', 'llm_model.model.layers.12.input_layernorm.weight', 'llm_model.model.layers.12.post_attention_layernorm.weight', 'llm_model.model.layers.13.self_attn.q_proj.weight', 'llm_model.model.layers.13.self_attn.k_proj.weight', 'llm_model.model.layers.13.self_attn.v_proj.weight', 'llm_model.model.layers.13.self_attn.o_proj.weight', 'llm_model.model.layers.13.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.13.mlp.gate_proj.weight', 'llm_model.model.layers.13.mlp.down_proj.weight', 'llm_model.model.layers.13.mlp.up_proj.weight', 'llm_model.model.layers.13.input_layernorm.weight', 'llm_model.model.layers.13.post_attention_layernorm.weight', 'llm_model.model.layers.14.self_attn.q_proj.weight', 'llm_model.model.layers.14.self_attn.k_proj.weight', 'llm_model.model.layers.14.self_attn.v_proj.weight', 'llm_model.model.layers.14.self_attn.o_proj.weight', 'llm_model.model.layers.14.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.14.mlp.gate_proj.weight', 'llm_model.model.layers.14.mlp.down_proj.weight', 'llm_model.model.layers.14.mlp.up_proj.weight', 'llm_model.model.layers.14.input_layernorm.weight', 'llm_model.model.layers.14.post_attention_layernorm.weight', 'llm_model.model.layers.15.self_attn.q_proj.weight', 'llm_model.model.layers.15.self_attn.k_proj.weight', 'llm_model.model.layers.15.self_attn.v_proj.weight', 'llm_model.model.layers.15.self_attn.o_proj.weight', 'llm_model.model.layers.15.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.15.mlp.gate_proj.weight', 'llm_model.model.layers.15.mlp.down_proj.weight', 'llm_model.model.layers.15.mlp.up_proj.weight', 'llm_model.model.layers.15.input_layernorm.weight', 'llm_model.model.layers.15.post_attention_layernorm.weight', 'llm_model.model.layers.16.self_attn.q_proj.weight', 'llm_model.model.layers.16.self_attn.k_proj.weight', 'llm_model.model.layers.16.self_attn.v_proj.weight', 'llm_model.model.layers.16.self_attn.o_proj.weight', 'llm_model.model.layers.16.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.16.mlp.gate_proj.weight', 'llm_model.model.layers.16.mlp.down_proj.weight', 'llm_model.model.layers.16.mlp.up_proj.weight', 'llm_model.model.layers.16.input_layernorm.weight', 'llm_model.model.layers.16.post_attention_layernorm.weight', 'llm_model.model.layers.17.self_attn.q_proj.weight', 'llm_model.model.layers.17.self_attn.k_proj.weight', 'llm_model.model.layers.17.self_attn.v_proj.weight', 'llm_model.model.layers.17.self_attn.o_proj.weight', 'llm_model.model.layers.17.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.17.mlp.gate_proj.weight', 'llm_model.model.layers.17.mlp.down_proj.weight', 'llm_model.model.layers.17.mlp.up_proj.weight', 'llm_model.model.layers.17.input_layernorm.weight', 'llm_model.model.layers.17.post_attention_layernorm.weight', 'llm_model.model.layers.18.self_attn.q_proj.weight', 'llm_model.model.layers.18.self_attn.k_proj.weight', 'llm_model.model.layers.18.self_attn.v_proj.weight', 'llm_model.model.layers.18.self_attn.o_proj.weight', 'llm_model.model.layers.18.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.18.mlp.gate_proj.weight', 'llm_model.model.layers.18.mlp.down_proj.weight', 'llm_model.model.layers.18.mlp.up_proj.weight', 'llm_model.model.layers.18.input_layernorm.weight', 'llm_model.model.layers.18.post_attention_layernorm.weight', 'llm_model.model.layers.19.self_attn.q_proj.weight', 'llm_model.model.layers.19.self_attn.k_proj.weight', 'llm_model.model.layers.19.self_attn.v_proj.weight', 'llm_model.model.layers.19.self_attn.o_proj.weight', 'llm_model.model.layers.19.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.19.mlp.gate_proj.weight', 'llm_model.model.layers.19.mlp.down_proj.weight', 'llm_model.model.layers.19.mlp.up_proj.weight', 'llm_model.model.layers.19.input_layernorm.weight', 'llm_model.model.layers.19.post_attention_layernorm.weight', 'llm_model.model.layers.20.self_attn.q_proj.weight', 'llm_model.model.layers.20.self_attn.k_proj.weight', 'llm_model.model.layers.20.self_attn.v_proj.weight', 'llm_model.model.layers.20.self_attn.o_proj.weight', 'llm_model.model.layers.20.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.20.mlp.gate_proj.weight', 'llm_model.model.layers.20.mlp.down_proj.weight', 'llm_model.model.layers.20.mlp.up_proj.weight', 'llm_model.model.layers.20.input_layernorm.weight', 'llm_model.model.layers.20.post_attention_layernorm.weight', 'llm_model.model.layers.21.self_attn.q_proj.weight', 'llm_model.model.layers.21.self_attn.k_proj.weight', 'llm_model.model.layers.21.self_attn.v_proj.weight', 'llm_model.model.layers.21.self_attn.o_proj.weight', 'llm_model.model.layers.21.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.21.mlp.gate_proj.weight', 'llm_model.model.layers.21.mlp.down_proj.weight', 'llm_model.model.layers.21.mlp.up_proj.weight', 'llm_model.model.layers.21.input_layernorm.weight', 'llm_model.model.layers.21.post_attention_layernorm.weight', 'llm_model.model.layers.22.self_attn.q_proj.weight', 'llm_model.model.layers.22.self_attn.k_proj.weight', 'llm_model.model.layers.22.self_attn.v_proj.weight', 'llm_model.model.layers.22.self_attn.o_proj.weight', 'llm_model.model.layers.22.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.22.mlp.gate_proj.weight', 'llm_model.model.layers.22.mlp.down_proj.weight', 'llm_model.model.layers.22.mlp.up_proj.weight', 'llm_model.model.layers.22.input_layernorm.weight', 'llm_model.model.layers.22.post_attention_layernorm.weight', 'llm_model.model.layers.23.self_attn.q_proj.weight', 'llm_model.model.layers.23.self_attn.k_proj.weight', 'llm_model.model.layers.23.self_attn.v_proj.weight', 'llm_model.model.layers.23.self_attn.o_proj.weight', 'llm_model.model.layers.23.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.23.mlp.gate_proj.weight', 'llm_model.model.layers.23.mlp.down_proj.weight', 'llm_model.model.layers.23.mlp.up_proj.weight', 'llm_model.model.layers.23.input_layernorm.weight', 'llm_model.model.layers.23.post_attention_layernorm.weight', 'llm_model.model.layers.24.self_attn.q_proj.weight', 'llm_model.model.layers.24.self_attn.k_proj.weight', 'llm_model.model.layers.24.self_attn.v_proj.weight', 'llm_model.model.layers.24.self_attn.o_proj.weight', 'llm_model.model.layers.24.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.24.mlp.gate_proj.weight', 'llm_model.model.layers.24.mlp.down_proj.weight', 'llm_model.model.layers.24.mlp.up_proj.weight', 'llm_model.model.layers.24.input_layernorm.weight', 'llm_model.model.layers.24.post_attention_layernorm.weight', 'llm_model.model.layers.25.self_attn.q_proj.weight', 'llm_model.model.layers.25.self_attn.k_proj.weight', 'llm_model.model.layers.25.self_attn.v_proj.weight', 'llm_model.model.layers.25.self_attn.o_proj.weight', 'llm_model.model.layers.25.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.25.mlp.gate_proj.weight', 'llm_model.model.layers.25.mlp.down_proj.weight', 'llm_model.model.layers.25.mlp.up_proj.weight', 'llm_model.model.layers.25.input_layernorm.weight', 'llm_model.model.layers.25.post_attention_layernorm.weight', 'llm_model.model.layers.26.self_attn.q_proj.weight', 'llm_model.model.layers.26.self_attn.k_proj.weight', 'llm_model.model.layers.26.self_attn.v_proj.weight', 'llm_model.model.layers.26.self_attn.o_proj.weight', 'llm_model.model.layers.26.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.26.mlp.gate_proj.weight', 'llm_model.model.layers.26.mlp.down_proj.weight', 'llm_model.model.layers.26.mlp.up_proj.weight', 'llm_model.model.layers.26.input_layernorm.weight', 'llm_model.model.layers.26.post_attention_layernorm.weight', 'llm_model.model.layers.27.self_attn.q_proj.weight', 'llm_model.model.layers.27.self_attn.k_proj.weight', 'llm_model.model.layers.27.self_attn.v_proj.weight', 'llm_model.model.layers.27.self_attn.o_proj.weight', 'llm_model.model.layers.27.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.27.mlp.gate_proj.weight', 'llm_model.model.layers.27.mlp.down_proj.weight', 'llm_model.model.layers.27.mlp.up_proj.weight', 'llm_model.model.layers.27.input_layernorm.weight', 'llm_model.model.layers.27.post_attention_layernorm.weight', 'llm_model.model.layers.28.self_attn.q_proj.weight', 'llm_model.model.layers.28.self_attn.k_proj.weight', 'llm_model.model.layers.28.self_attn.v_proj.weight', 'llm_model.model.layers.28.self_attn.o_proj.weight', 'llm_model.model.layers.28.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.28.mlp.gate_proj.weight', 'llm_model.model.layers.28.mlp.down_proj.weight', 'llm_model.model.layers.28.mlp.up_proj.weight', 'llm_model.model.layers.28.input_layernorm.weight', 'llm_model.model.layers.28.post_attention_layernorm.weight', 'llm_model.model.layers.29.self_attn.q_proj.weight', 'llm_model.model.layers.29.self_attn.k_proj.weight', 'llm_model.model.layers.29.self_attn.v_proj.weight', 'llm_model.model.layers.29.self_attn.o_proj.weight', 'llm_model.model.layers.29.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.29.mlp.gate_proj.weight', 'llm_model.model.layers.29.mlp.down_proj.weight', 'llm_model.model.layers.29.mlp.up_proj.weight', 'llm_model.model.layers.29.input_layernorm.weight', 'llm_model.model.layers.29.post_attention_layernorm.weight', 'llm_model.model.layers.30.self_attn.q_proj.weight', 'llm_model.model.layers.30.self_attn.k_proj.weight', 'llm_model.model.layers.30.self_attn.v_proj.weight', 'llm_model.model.layers.30.self_attn.o_proj.weight', 'llm_model.model.layers.30.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.30.mlp.gate_proj.weight', 'llm_model.model.layers.30.mlp.down_proj.weight', 'llm_model.model.layers.30.mlp.up_proj.weight', 'llm_model.model.layers.30.input_layernorm.weight', 'llm_model.model.layers.30.post_attention_layernorm.weight', 'llm_model.model.layers.31.self_attn.q_proj.weight', 'llm_model.model.layers.31.self_attn.k_proj.weight', 'llm_model.model.layers.31.self_attn.v_proj.weight', 'llm_model.model.layers.31.self_attn.o_proj.weight', 'llm_model.model.layers.31.self_attn.rotary_emb.inv_freq', 'llm_model.model.layers.31.mlp.gate_proj.weight', 'llm_model.model.layers.31.mlp.down_proj.weight', 'llm_model.model.layers.31.mlp.up_proj.weight', 'llm_model.model.layers.31.input_layernorm.weight', 'llm_model.model.layers.31.post_attention_layernorm.weight', 'llm_model.model.norm.weight', 'llm_model.model.train_embed_tokens.weight', 'llm_model.lm_head.weight']\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth\n",
      "Missing keys ['query_tokens', 'visual_encoder.cls_token', 'visual_encoder.pos_embed', 'visual_encoder.patch_embed.proj.weight', 'visual_encoder.patch_embed.proj.bias', 'visual_encoder.blocks.0.norm1.weight', 'visual_encoder.blocks.0.norm1.bias', 'visual_encoder.blocks.0.attn.q_bias', 'visual_encoder.blocks.0.attn.v_bias', 'visual_encoder.blocks.0.attn.qkv.weight', 'visual_encoder.blocks.0.attn.proj.weight', 'visual_encoder.blocks.0.attn.proj.bias', 'visual_encoder.blocks.0.norm2.weight', 'visual_encoder.blocks.0.norm2.bias', 'visual_encoder.blocks.0.mlp.fc1.weight', 'visual_encoder.blocks.0.mlp.fc1.bias', 'visual_encoder.blocks.0.mlp.fc2.weight', 'visual_encoder.blocks.0.mlp.fc2.bias', 'visual_encoder.blocks.1.norm1.weight', 'visual_encoder.blocks.1.norm1.bias', 'visual_encoder.blocks.1.attn.q_bias', 'visual_encoder.blocks.1.attn.v_bias', 'visual_encoder.blocks.1.attn.qkv.weight', 'visual_encoder.blocks.1.attn.proj.weight', 'visual_encoder.blocks.1.attn.proj.bias', 'visual_encoder.blocks.1.norm2.weight', 'visual_encoder.blocks.1.norm2.bias', 'visual_encoder.blocks.1.mlp.fc1.weight', 'visual_encoder.blocks.1.mlp.fc1.bias', 'visual_encoder.blocks.1.mlp.fc2.weight', 'visual_encoder.blocks.1.mlp.fc2.bias', 'visual_encoder.blocks.2.norm1.weight', 'visual_encoder.blocks.2.norm1.bias', 'visual_encoder.blocks.2.attn.q_bias', 'visual_encoder.blocks.2.attn.v_bias', 'visual_encoder.blocks.2.attn.qkv.weight', 'visual_encoder.blocks.2.attn.proj.weight', 'visual_encoder.blocks.2.attn.proj.bias', 'visual_encoder.blocks.2.norm2.weight', 'visual_encoder.blocks.2.norm2.bias', 'visual_encoder.blocks.2.mlp.fc1.weight', 'visual_encoder.blocks.2.mlp.fc1.bias', 'visual_encoder.blocks.2.mlp.fc2.weight', 'visual_encoder.blocks.2.mlp.fc2.bias', 'visual_encoder.blocks.3.norm1.weight', 'visual_encoder.blocks.3.norm1.bias', 'visual_encoder.blocks.3.attn.q_bias', 'visual_encoder.blocks.3.attn.v_bias', 'visual_encoder.blocks.3.attn.qkv.weight', 'visual_encoder.blocks.3.attn.proj.weight', 'visual_encoder.blocks.3.attn.proj.bias', 'visual_encoder.blocks.3.norm2.weight', 'visual_encoder.blocks.3.norm2.bias', 'visual_encoder.blocks.3.mlp.fc1.weight', 'visual_encoder.blocks.3.mlp.fc1.bias', 'visual_encoder.blocks.3.mlp.fc2.weight', 'visual_encoder.blocks.3.mlp.fc2.bias', 'visual_encoder.blocks.4.norm1.weight', 'visual_encoder.blocks.4.norm1.bias', 'visual_encoder.blocks.4.attn.q_bias', 'visual_encoder.blocks.4.attn.v_bias', 'visual_encoder.blocks.4.attn.qkv.weight', 'visual_encoder.blocks.4.attn.proj.weight', 'visual_encoder.blocks.4.attn.proj.bias', 'visual_encoder.blocks.4.norm2.weight', 'visual_encoder.blocks.4.norm2.bias', 'visual_encoder.blocks.4.mlp.fc1.weight', 'visual_encoder.blocks.4.mlp.fc1.bias', 'visual_encoder.blocks.4.mlp.fc2.weight', 'visual_encoder.blocks.4.mlp.fc2.bias', 'visual_encoder.blocks.5.norm1.weight', 'visual_encoder.blocks.5.norm1.bias', 'visual_encoder.blocks.5.attn.q_bias', 'visual_encoder.blocks.5.attn.v_bias', 'visual_encoder.blocks.5.attn.qkv.weight', 'visual_encoder.blocks.5.attn.proj.weight', 'visual_encoder.blocks.5.attn.proj.bias', 'visual_encoder.blocks.5.norm2.weight', 'visual_encoder.blocks.5.norm2.bias', 'visual_encoder.blocks.5.mlp.fc1.weight', 'visual_encoder.blocks.5.mlp.fc1.bias', 'visual_encoder.blocks.5.mlp.fc2.weight', 'visual_encoder.blocks.5.mlp.fc2.bias', 'visual_encoder.blocks.6.norm1.weight', 'visual_encoder.blocks.6.norm1.bias', 'visual_encoder.blocks.6.attn.q_bias', 'visual_encoder.blocks.6.attn.v_bias', 'visual_encoder.blocks.6.attn.qkv.weight', 'visual_encoder.blocks.6.attn.proj.weight', 'visual_encoder.blocks.6.attn.proj.bias', 'visual_encoder.blocks.6.norm2.weight', 'visual_encoder.blocks.6.norm2.bias', 'visual_encoder.blocks.6.mlp.fc1.weight', 'visual_encoder.blocks.6.mlp.fc1.bias', 'visual_encoder.blocks.6.mlp.fc2.weight', 'visual_encoder.blocks.6.mlp.fc2.bias', 'visual_encoder.blocks.7.norm1.weight', 'visual_encoder.blocks.7.norm1.bias', 'visual_encoder.blocks.7.attn.q_bias', 'visual_encoder.blocks.7.attn.v_bias', 'visual_encoder.blocks.7.attn.qkv.weight', 'visual_encoder.blocks.7.attn.proj.weight', 'visual_encoder.blocks.7.attn.proj.bias', 'visual_encoder.blocks.7.norm2.weight', 'visual_encoder.blocks.7.norm2.bias', 'visual_encoder.blocks.7.mlp.fc1.weight', 'visual_encoder.blocks.7.mlp.fc1.bias', 'visual_encoder.blocks.7.mlp.fc2.weight', 'visual_encoder.blocks.7.mlp.fc2.bias', 'visual_encoder.blocks.8.norm1.weight', 'visual_encoder.blocks.8.norm1.bias', 'visual_encoder.blocks.8.attn.q_bias', 'visual_encoder.blocks.8.attn.v_bias', 'visual_encoder.blocks.8.attn.qkv.weight', 'visual_encoder.blocks.8.attn.proj.weight', 'visual_encoder.blocks.8.attn.proj.bias', 'visual_encoder.blocks.8.norm2.weight', 'visual_encoder.blocks.8.norm2.bias', 'visual_encoder.blocks.8.mlp.fc1.weight', 'visual_encoder.blocks.8.mlp.fc1.bias', 'visual_encoder.blocks.8.mlp.fc2.weight', 'visual_encoder.blocks.8.mlp.fc2.bias', 'visual_encoder.blocks.9.norm1.weight', 'visual_encoder.blocks.9.norm1.bias', 'visual_encoder.blocks.9.attn.q_bias', 'visual_encoder.blocks.9.attn.v_bias', 'visual_encoder.blocks.9.attn.qkv.weight', 'visual_encoder.blocks.9.attn.proj.weight', 'visual_encoder.blocks.9.attn.proj.bias', 'visual_encoder.blocks.9.norm2.weight', 'visual_encoder.blocks.9.norm2.bias', 'visual_encoder.blocks.9.mlp.fc1.weight', 'visual_encoder.blocks.9.mlp.fc1.bias', 'visual_encoder.blocks.9.mlp.fc2.weight', 'visual_encoder.blocks.9.mlp.fc2.bias', 'visual_encoder.blocks.10.norm1.weight', 'visual_encoder.blocks.10.norm1.bias', 'visual_encoder.blocks.10.attn.q_bias', 'visual_encoder.blocks.10.attn.v_bias', 'visual_encoder.blocks.10.attn.qkv.weight', 'visual_encoder.blocks.10.attn.proj.weight', 'visual_encoder.blocks.10.attn.proj.bias', 'visual_encoder.blocks.10.norm2.weight', 'visual_encoder.blocks.10.norm2.bias', 'visual_encoder.blocks.10.mlp.fc1.weight', 'visual_encoder.blocks.10.mlp.fc1.bias', 'visual_encoder.blocks.10.mlp.fc2.weight', 'visual_encoder.blocks.10.mlp.fc2.bias', 'visual_encoder.blocks.11.norm1.weight', 'visual_encoder.blocks.11.norm1.bias', 'visual_encoder.blocks.11.attn.q_bias', 'visual_encoder.blocks.11.attn.v_bias', 'visual_encoder.blocks.11.attn.qkv.weight', 'visual_encoder.blocks.11.attn.proj.weight', 'visual_encoder.blocks.11.attn.proj.bias', 'visual_encoder.blocks.11.norm2.weight', 'visual_encoder.blocks.11.norm2.bias', 'visual_encoder.blocks.11.mlp.fc1.weight', 'visual_encoder.blocks.11.mlp.fc1.bias', 'visual_encoder.blocks.11.mlp.fc2.weight', 'visual_encoder.blocks.11.mlp.fc2.bias', 'visual_encoder.blocks.12.norm1.weight', 'visual_encoder.blocks.12.norm1.bias', 'visual_encoder.blocks.12.attn.q_bias', 'visual_encoder.blocks.12.attn.v_bias', 'visual_encoder.blocks.12.attn.qkv.weight', 'visual_encoder.blocks.12.attn.proj.weight', 'visual_encoder.blocks.12.attn.proj.bias', 'visual_encoder.blocks.12.norm2.weight', 'visual_encoder.blocks.12.norm2.bias', 'visual_encoder.blocks.12.mlp.fc1.weight', 'visual_encoder.blocks.12.mlp.fc1.bias', 'visual_encoder.blocks.12.mlp.fc2.weight', 'visual_encoder.blocks.12.mlp.fc2.bias', 'visual_encoder.blocks.13.norm1.weight', 'visual_encoder.blocks.13.norm1.bias', 'visual_encoder.blocks.13.attn.q_bias', 'visual_encoder.blocks.13.attn.v_bias', 'visual_encoder.blocks.13.attn.qkv.weight', 'visual_encoder.blocks.13.attn.proj.weight', 'visual_encoder.blocks.13.attn.proj.bias', 'visual_encoder.blocks.13.norm2.weight', 'visual_encoder.blocks.13.norm2.bias', 'visual_encoder.blocks.13.mlp.fc1.weight', 'visual_encoder.blocks.13.mlp.fc1.bias', 'visual_encoder.blocks.13.mlp.fc2.weight', 'visual_encoder.blocks.13.mlp.fc2.bias', 'visual_encoder.blocks.14.norm1.weight', 'visual_encoder.blocks.14.norm1.bias', 'visual_encoder.blocks.14.attn.q_bias', 'visual_encoder.blocks.14.attn.v_bias', 'visual_encoder.blocks.14.attn.qkv.weight', 'visual_encoder.blocks.14.attn.proj.weight', 'visual_encoder.blocks.14.attn.proj.bias', 'visual_encoder.blocks.14.norm2.weight', 'visual_encoder.blocks.14.norm2.bias', 'visual_encoder.blocks.14.mlp.fc1.weight', 'visual_encoder.blocks.14.mlp.fc1.bias', 'visual_encoder.blocks.14.mlp.fc2.weight', 'visual_encoder.blocks.14.mlp.fc2.bias', 'visual_encoder.blocks.15.norm1.weight', 'visual_encoder.blocks.15.norm1.bias', 'visual_encoder.blocks.15.attn.q_bias', 'visual_encoder.blocks.15.attn.v_bias', 'visual_encoder.blocks.15.attn.qkv.weight', 'visual_encoder.blocks.15.attn.proj.weight', 'visual_encoder.blocks.15.attn.proj.bias', 'visual_encoder.blocks.15.norm2.weight', 'visual_encoder.blocks.15.norm2.bias', 'visual_encoder.blocks.15.mlp.fc1.weight', 'visual_encoder.blocks.15.mlp.fc1.bias', 'visual_encoder.blocks.15.mlp.fc2.weight', 'visual_encoder.blocks.15.mlp.fc2.bias', 'visual_encoder.blocks.16.norm1.weight', 'visual_encoder.blocks.16.norm1.bias', 'visual_encoder.blocks.16.attn.q_bias', 'visual_encoder.blocks.16.attn.v_bias', 'visual_encoder.blocks.16.attn.qkv.weight', 'visual_encoder.blocks.16.attn.proj.weight', 'visual_encoder.blocks.16.attn.proj.bias', 'visual_encoder.blocks.16.norm2.weight', 'visual_encoder.blocks.16.norm2.bias', 'visual_encoder.blocks.16.mlp.fc1.weight', 'visual_encoder.blocks.16.mlp.fc1.bias', 'visual_encoder.blocks.16.mlp.fc2.weight', 'visual_encoder.blocks.16.mlp.fc2.bias', 'visual_encoder.blocks.17.norm1.weight', 'visual_encoder.blocks.17.norm1.bias', 'visual_encoder.blocks.17.attn.q_bias', 'visual_encoder.blocks.17.attn.v_bias', 'visual_encoder.blocks.17.attn.qkv.weight', 'visual_encoder.blocks.17.attn.proj.weight', 'visual_encoder.blocks.17.attn.proj.bias', 'visual_encoder.blocks.17.norm2.weight', 'visual_encoder.blocks.17.norm2.bias', 'visual_encoder.blocks.17.mlp.fc1.weight', 'visual_encoder.blocks.17.mlp.fc1.bias', 'visual_encoder.blocks.17.mlp.fc2.weight', 'visual_encoder.blocks.17.mlp.fc2.bias', 'visual_encoder.blocks.18.norm1.weight', 'visual_encoder.blocks.18.norm1.bias', 'visual_encoder.blocks.18.attn.q_bias', 'visual_encoder.blocks.18.attn.v_bias', 'visual_encoder.blocks.18.attn.qkv.weight', 'visual_encoder.blocks.18.attn.proj.weight', 'visual_encoder.blocks.18.attn.proj.bias', 'visual_encoder.blocks.18.norm2.weight', 'visual_encoder.blocks.18.norm2.bias', 'visual_encoder.blocks.18.mlp.fc1.weight', 'visual_encoder.blocks.18.mlp.fc1.bias', 'visual_encoder.blocks.18.mlp.fc2.weight', 'visual_encoder.blocks.18.mlp.fc2.bias', 'visual_encoder.blocks.19.norm1.weight', 'visual_encoder.blocks.19.norm1.bias', 'visual_encoder.blocks.19.attn.q_bias', 'visual_encoder.blocks.19.attn.v_bias', 'visual_encoder.blocks.19.attn.qkv.weight', 'visual_encoder.blocks.19.attn.proj.weight', 'visual_encoder.blocks.19.attn.proj.bias', 'visual_encoder.blocks.19.norm2.weight', 'visual_encoder.blocks.19.norm2.bias', 'visual_encoder.blocks.19.mlp.fc1.weight', 'visual_encoder.blocks.19.mlp.fc1.bias', 'visual_encoder.blocks.19.mlp.fc2.weight', 'visual_encoder.blocks.19.mlp.fc2.bias', 'visual_encoder.blocks.20.norm1.weight', 'visual_encoder.blocks.20.norm1.bias', 'visual_encoder.blocks.20.attn.q_bias', 'visual_encoder.blocks.20.attn.v_bias', 'visual_encoder.blocks.20.attn.qkv.weight', 'visual_encoder.blocks.20.attn.proj.weight', 'visual_encoder.blocks.20.attn.proj.bias', 'visual_encoder.blocks.20.norm2.weight', 'visual_encoder.blocks.20.norm2.bias', 'visual_encoder.blocks.20.mlp.fc1.weight', 'visual_encoder.blocks.20.mlp.fc1.bias', 'visual_encoder.blocks.20.mlp.fc2.weight', 'visual_encoder.blocks.20.mlp.fc2.bias', 'visual_encoder.blocks.21.norm1.weight', 'visual_encoder.blocks.21.norm1.bias', 'visual_encoder.blocks.21.attn.q_bias', 'visual_encoder.blocks.21.attn.v_bias', 'visual_encoder.blocks.21.attn.qkv.weight', 'visual_encoder.blocks.21.attn.proj.weight', 'visual_encoder.blocks.21.attn.proj.bias', 'visual_encoder.blocks.21.norm2.weight', 'visual_encoder.blocks.21.norm2.bias', 'visual_encoder.blocks.21.mlp.fc1.weight', 'visual_encoder.blocks.21.mlp.fc1.bias', 'visual_encoder.blocks.21.mlp.fc2.weight', 'visual_encoder.blocks.21.mlp.fc2.bias', 'visual_encoder.blocks.22.norm1.weight', 'visual_encoder.blocks.22.norm1.bias', 'visual_encoder.blocks.22.attn.q_bias', 'visual_encoder.blocks.22.attn.v_bias', 'visual_encoder.blocks.22.attn.qkv.weight', 'visual_encoder.blocks.22.attn.proj.weight', 'visual_encoder.blocks.22.attn.proj.bias', 'visual_encoder.blocks.22.norm2.weight', 'visual_encoder.blocks.22.norm2.bias', 'visual_encoder.blocks.22.mlp.fc1.weight', 'visual_encoder.blocks.22.mlp.fc1.bias', 'visual_encoder.blocks.22.mlp.fc2.weight', 'visual_encoder.blocks.22.mlp.fc2.bias', 'visual_encoder.blocks.23.norm1.weight', 'visual_encoder.blocks.23.norm1.bias', 'visual_encoder.blocks.23.attn.q_bias', 'visual_encoder.blocks.23.attn.v_bias', 'visual_encoder.blocks.23.attn.qkv.weight', 'visual_encoder.blocks.23.attn.proj.weight', 'visual_encoder.blocks.23.attn.proj.bias', 'visual_encoder.blocks.23.norm2.weight', 'visual_encoder.blocks.23.norm2.bias', 'visual_encoder.blocks.23.mlp.fc1.weight', 'visual_encoder.blocks.23.mlp.fc1.bias', 'visual_encoder.blocks.23.mlp.fc2.weight', 'visual_encoder.blocks.23.mlp.fc2.bias', 'visual_encoder.blocks.24.norm1.weight', 'visual_encoder.blocks.24.norm1.bias', 'visual_encoder.blocks.24.attn.q_bias', 'visual_encoder.blocks.24.attn.v_bias', 'visual_encoder.blocks.24.attn.qkv.weight', 'visual_encoder.blocks.24.attn.proj.weight', 'visual_encoder.blocks.24.attn.proj.bias', 'visual_encoder.blocks.24.norm2.weight', 'visual_encoder.blocks.24.norm2.bias', 'visual_encoder.blocks.24.mlp.fc1.weight', 'visual_encoder.blocks.24.mlp.fc1.bias', 'visual_encoder.blocks.24.mlp.fc2.weight', 'visual_encoder.blocks.24.mlp.fc2.bias', 'visual_encoder.blocks.25.norm1.weight', 'visual_encoder.blocks.25.norm1.bias', 'visual_encoder.blocks.25.attn.q_bias', 'visual_encoder.blocks.25.attn.v_bias', 'visual_encoder.blocks.25.attn.qkv.weight', 'visual_encoder.blocks.25.attn.proj.weight', 'visual_encoder.blocks.25.attn.proj.bias', 'visual_encoder.blocks.25.norm2.weight', 'visual_encoder.blocks.25.norm2.bias', 'visual_encoder.blocks.25.mlp.fc1.weight', 'visual_encoder.blocks.25.mlp.fc1.bias', 'visual_encoder.blocks.25.mlp.fc2.weight', 'visual_encoder.blocks.25.mlp.fc2.bias', 'visual_encoder.blocks.26.norm1.weight', 'visual_encoder.blocks.26.norm1.bias', 'visual_encoder.blocks.26.attn.q_bias', 'visual_encoder.blocks.26.attn.v_bias', 'visual_encoder.blocks.26.attn.qkv.weight', 'visual_encoder.blocks.26.attn.proj.weight', 'visual_encoder.blocks.26.attn.proj.bias', 'visual_encoder.blocks.26.norm2.weight', 'visual_encoder.blocks.26.norm2.bias', 'visual_encoder.blocks.26.mlp.fc1.weight', 'visual_encoder.blocks.26.mlp.fc1.bias', 'visual_encoder.blocks.26.mlp.fc2.weight', 'visual_encoder.blocks.26.mlp.fc2.bias', 'visual_encoder.blocks.27.norm1.weight', 'visual_encoder.blocks.27.norm1.bias', 'visual_encoder.blocks.27.attn.q_bias', 'visual_encoder.blocks.27.attn.v_bias', 'visual_encoder.blocks.27.attn.qkv.weight', 'visual_encoder.blocks.27.attn.proj.weight', 'visual_encoder.blocks.27.attn.proj.bias', 'visual_encoder.blocks.27.norm2.weight', 'visual_encoder.blocks.27.norm2.bias', 'visual_encoder.blocks.27.mlp.fc1.weight', 'visual_encoder.blocks.27.mlp.fc1.bias', 'visual_encoder.blocks.27.mlp.fc2.weight', 'visual_encoder.blocks.27.mlp.fc2.bias', 'visual_encoder.blocks.28.norm1.weight', 'visual_encoder.blocks.28.norm1.bias', 'visual_encoder.blocks.28.attn.q_bias', 'visual_encoder.blocks.28.attn.v_bias', 'visual_encoder.blocks.28.attn.qkv.weight', 'visual_encoder.blocks.28.attn.proj.weight', 'visual_encoder.blocks.28.attn.proj.bias', 'visual_encoder.blocks.28.norm2.weight', 'visual_encoder.blocks.28.norm2.bias', 'visual_encoder.blocks.28.mlp.fc1.weight', 'visual_encoder.blocks.28.mlp.fc1.bias', 'visual_encoder.blocks.28.mlp.fc2.weight', 'visual_encoder.blocks.28.mlp.fc2.bias', 'visual_encoder.blocks.29.norm1.weight', 'visual_encoder.blocks.29.norm1.bias', 'visual_encoder.blocks.29.attn.q_bias', 'visual_encoder.blocks.29.attn.v_bias', 'visual_encoder.blocks.29.attn.qkv.weight', 'visual_encoder.blocks.29.attn.proj.weight', 'visual_encoder.blocks.29.attn.proj.bias', 'visual_encoder.blocks.29.norm2.weight', 'visual_encoder.blocks.29.norm2.bias', 'visual_encoder.blocks.29.mlp.fc1.weight', 'visual_encoder.blocks.29.mlp.fc1.bias', 'visual_encoder.blocks.29.mlp.fc2.weight', 'visual_encoder.blocks.29.mlp.fc2.bias', 'visual_encoder.blocks.30.norm1.weight', 'visual_encoder.blocks.30.norm1.bias', 'visual_encoder.blocks.30.attn.q_bias', 'visual_encoder.blocks.30.attn.v_bias', 'visual_encoder.blocks.30.attn.qkv.weight', 'visual_encoder.blocks.30.attn.proj.weight', 'visual_encoder.blocks.30.attn.proj.bias', 'visual_encoder.blocks.30.norm2.weight', 'visual_encoder.blocks.30.norm2.bias', 'visual_encoder.blocks.30.mlp.fc1.weight', 'visual_encoder.blocks.30.mlp.fc1.bias', 'visual_encoder.blocks.30.mlp.fc2.weight', 'visual_encoder.blocks.30.mlp.fc2.bias', 'visual_encoder.blocks.31.norm1.weight', 'visual_encoder.blocks.31.norm1.bias', 'visual_encoder.blocks.31.attn.q_bias', 'visual_encoder.blocks.31.attn.v_bias', 'visual_encoder.blocks.31.attn.qkv.weight', 'visual_encoder.blocks.31.attn.proj.weight', 'visual_encoder.blocks.31.attn.proj.bias', 'visual_encoder.blocks.31.norm2.weight', 'visual_encoder.blocks.31.norm2.bias', 'visual_encoder.blocks.31.mlp.fc1.weight', 'visual_encoder.blocks.31.mlp.fc1.bias', 'visual_encoder.blocks.31.mlp.fc2.weight', 'visual_encoder.blocks.31.mlp.fc2.bias', 'visual_encoder.blocks.32.norm1.weight', 'visual_encoder.blocks.32.norm1.bias', 'visual_encoder.blocks.32.attn.q_bias', 'visual_encoder.blocks.32.attn.v_bias', 'visual_encoder.blocks.32.attn.qkv.weight', 'visual_encoder.blocks.32.attn.proj.weight', 'visual_encoder.blocks.32.attn.proj.bias', 'visual_encoder.blocks.32.norm2.weight', 'visual_encoder.blocks.32.norm2.bias', 'visual_encoder.blocks.32.mlp.fc1.weight', 'visual_encoder.blocks.32.mlp.fc1.bias', 'visual_encoder.blocks.32.mlp.fc2.weight', 'visual_encoder.blocks.32.mlp.fc2.bias', 'visual_encoder.blocks.33.norm1.weight', 'visual_encoder.blocks.33.norm1.bias', 'visual_encoder.blocks.33.attn.q_bias', 'visual_encoder.blocks.33.attn.v_bias', 'visual_encoder.blocks.33.attn.qkv.weight', 'visual_encoder.blocks.33.attn.proj.weight', 'visual_encoder.blocks.33.attn.proj.bias', 'visual_encoder.blocks.33.norm2.weight', 'visual_encoder.blocks.33.norm2.bias', 'visual_encoder.blocks.33.mlp.fc1.weight', 'visual_encoder.blocks.33.mlp.fc1.bias', 'visual_encoder.blocks.33.mlp.fc2.weight', 'visual_encoder.blocks.33.mlp.fc2.bias', 'visual_encoder.blocks.34.norm1.weight', 'visual_encoder.blocks.34.norm1.bias', 'visual_encoder.blocks.34.attn.q_bias', 'visual_encoder.blocks.34.attn.v_bias', 'visual_encoder.blocks.34.attn.qkv.weight', 'visual_encoder.blocks.34.attn.proj.weight', 'visual_encoder.blocks.34.attn.proj.bias', 'visual_encoder.blocks.34.norm2.weight', 'visual_encoder.blocks.34.norm2.bias', 'visual_encoder.blocks.34.mlp.fc1.weight', 'visual_encoder.blocks.34.mlp.fc1.bias', 'visual_encoder.blocks.34.mlp.fc2.weight', 'visual_encoder.blocks.34.mlp.fc2.bias', 'visual_encoder.blocks.35.norm1.weight', 'visual_encoder.blocks.35.norm1.bias', 'visual_encoder.blocks.35.attn.q_bias', 'visual_encoder.blocks.35.attn.v_bias', 'visual_encoder.blocks.35.attn.qkv.weight', 'visual_encoder.blocks.35.attn.proj.weight', 'visual_encoder.blocks.35.attn.proj.bias', 'visual_encoder.blocks.35.norm2.weight', 'visual_encoder.blocks.35.norm2.bias', 'visual_encoder.blocks.35.mlp.fc1.weight', 'visual_encoder.blocks.35.mlp.fc1.bias', 'visual_encoder.blocks.35.mlp.fc2.weight', 'visual_encoder.blocks.35.mlp.fc2.bias', 'visual_encoder.blocks.36.norm1.weight', 'visual_encoder.blocks.36.norm1.bias', 'visual_encoder.blocks.36.attn.q_bias', 'visual_encoder.blocks.36.attn.v_bias', 'visual_encoder.blocks.36.attn.qkv.weight', 'visual_encoder.blocks.36.attn.proj.weight', 'visual_encoder.blocks.36.attn.proj.bias', 'visual_encoder.blocks.36.norm2.weight', 'visual_encoder.blocks.36.norm2.bias', 'visual_encoder.blocks.36.mlp.fc1.weight', 'visual_encoder.blocks.36.mlp.fc1.bias', 'visual_encoder.blocks.36.mlp.fc2.weight', 'visual_encoder.blocks.36.mlp.fc2.bias', 'visual_encoder.blocks.37.norm1.weight', 'visual_encoder.blocks.37.norm1.bias', 'visual_encoder.blocks.37.attn.q_bias', 'visual_encoder.blocks.37.attn.v_bias', 'visual_encoder.blocks.37.attn.qkv.weight', 'visual_encoder.blocks.37.attn.proj.weight', 'visual_encoder.blocks.37.attn.proj.bias', 'visual_encoder.blocks.37.norm2.weight', 'visual_encoder.blocks.37.norm2.bias', 'visual_encoder.blocks.37.mlp.fc1.weight', 'visual_encoder.blocks.37.mlp.fc1.bias', 'visual_encoder.blocks.37.mlp.fc2.weight', 'visual_encoder.blocks.37.mlp.fc2.bias', 'visual_encoder.blocks.38.norm1.weight', 'visual_encoder.blocks.38.norm1.bias', 'visual_encoder.blocks.38.attn.q_bias', 'visual_encoder.blocks.38.attn.v_bias', 'visual_encoder.blocks.38.attn.qkv.weight', 'visual_encoder.blocks.38.attn.proj.weight', 'visual_encoder.blocks.38.attn.proj.bias', 'visual_encoder.blocks.38.norm2.weight', 'visual_encoder.blocks.38.norm2.bias', 'visual_encoder.blocks.38.mlp.fc1.weight', 'visual_encoder.blocks.38.mlp.fc1.bias', 'visual_encoder.blocks.38.mlp.fc2.weight', 'visual_encoder.blocks.38.mlp.fc2.bias', 'ln_vision.weight', 'ln_vision.bias', 'Qformer.bert.embeddings.word_embeddings.weight', 'Qformer.bert.embeddings.position_embeddings.weight', 'Qformer.bert.embeddings.LayerNorm.weight', 'Qformer.bert.embeddings.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.attention.self.query.weight', 'Qformer.bert.encoder.layer.0.attention.self.query.bias', 'Qformer.bert.encoder.layer.0.attention.self.key.weight', 'Qformer.bert.encoder.layer.0.attention.self.key.bias', 'Qformer.bert.encoder.layer.0.attention.self.value.weight', 'Qformer.bert.encoder.layer.0.attention.self.value.bias', 'Qformer.bert.encoder.layer.0.attention.output.dense.weight', 'Qformer.bert.encoder.layer.0.attention.output.dense.bias', 'Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.0.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.0.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.0.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.0.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.0.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.0.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.0.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.intermediate.dense.weight', 'Qformer.bert.encoder.layer.0.intermediate.dense.bias', 'Qformer.bert.encoder.layer.0.output.dense.weight', 'Qformer.bert.encoder.layer.0.output.dense.bias', 'Qformer.bert.encoder.layer.0.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.0.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.0.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.0.output_query.dense.weight', 'Qformer.bert.encoder.layer.0.output_query.dense.bias', 'Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.1.attention.self.query.weight', 'Qformer.bert.encoder.layer.1.attention.self.query.bias', 'Qformer.bert.encoder.layer.1.attention.self.key.weight', 'Qformer.bert.encoder.layer.1.attention.self.key.bias', 'Qformer.bert.encoder.layer.1.attention.self.value.weight', 'Qformer.bert.encoder.layer.1.attention.self.value.bias', 'Qformer.bert.encoder.layer.1.attention.output.dense.weight', 'Qformer.bert.encoder.layer.1.attention.output.dense.bias', 'Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.1.intermediate.dense.weight', 'Qformer.bert.encoder.layer.1.intermediate.dense.bias', 'Qformer.bert.encoder.layer.1.output.dense.weight', 'Qformer.bert.encoder.layer.1.output.dense.bias', 'Qformer.bert.encoder.layer.1.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.1.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.1.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.1.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.1.output_query.dense.weight', 'Qformer.bert.encoder.layer.1.output_query.dense.bias', 'Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.attention.self.query.weight', 'Qformer.bert.encoder.layer.2.attention.self.query.bias', 'Qformer.bert.encoder.layer.2.attention.self.key.weight', 'Qformer.bert.encoder.layer.2.attention.self.key.bias', 'Qformer.bert.encoder.layer.2.attention.self.value.weight', 'Qformer.bert.encoder.layer.2.attention.self.value.bias', 'Qformer.bert.encoder.layer.2.attention.output.dense.weight', 'Qformer.bert.encoder.layer.2.attention.output.dense.bias', 'Qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.2.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.2.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.2.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.2.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.2.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.2.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.2.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.intermediate.dense.weight', 'Qformer.bert.encoder.layer.2.intermediate.dense.bias', 'Qformer.bert.encoder.layer.2.output.dense.weight', 'Qformer.bert.encoder.layer.2.output.dense.bias', 'Qformer.bert.encoder.layer.2.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.2.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.2.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.2.output_query.dense.weight', 'Qformer.bert.encoder.layer.2.output_query.dense.bias', 'Qformer.bert.encoder.layer.2.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.2.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.3.attention.self.query.weight', 'Qformer.bert.encoder.layer.3.attention.self.query.bias', 'Qformer.bert.encoder.layer.3.attention.self.key.weight', 'Qformer.bert.encoder.layer.3.attention.self.key.bias', 'Qformer.bert.encoder.layer.3.attention.self.value.weight', 'Qformer.bert.encoder.layer.3.attention.self.value.bias', 'Qformer.bert.encoder.layer.3.attention.output.dense.weight', 'Qformer.bert.encoder.layer.3.attention.output.dense.bias', 'Qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.3.intermediate.dense.weight', 'Qformer.bert.encoder.layer.3.intermediate.dense.bias', 'Qformer.bert.encoder.layer.3.output.dense.weight', 'Qformer.bert.encoder.layer.3.output.dense.bias', 'Qformer.bert.encoder.layer.3.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.3.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.3.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.3.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.3.output_query.dense.weight', 'Qformer.bert.encoder.layer.3.output_query.dense.bias', 'Qformer.bert.encoder.layer.3.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.3.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.attention.self.query.weight', 'Qformer.bert.encoder.layer.4.attention.self.query.bias', 'Qformer.bert.encoder.layer.4.attention.self.key.weight', 'Qformer.bert.encoder.layer.4.attention.self.key.bias', 'Qformer.bert.encoder.layer.4.attention.self.value.weight', 'Qformer.bert.encoder.layer.4.attention.self.value.bias', 'Qformer.bert.encoder.layer.4.attention.output.dense.weight', 'Qformer.bert.encoder.layer.4.attention.output.dense.bias', 'Qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.4.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.4.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.4.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.4.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.4.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.4.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.4.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.intermediate.dense.weight', 'Qformer.bert.encoder.layer.4.intermediate.dense.bias', 'Qformer.bert.encoder.layer.4.output.dense.weight', 'Qformer.bert.encoder.layer.4.output.dense.bias', 'Qformer.bert.encoder.layer.4.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.4.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.4.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.4.output_query.dense.weight', 'Qformer.bert.encoder.layer.4.output_query.dense.bias', 'Qformer.bert.encoder.layer.4.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.4.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.5.attention.self.query.weight', 'Qformer.bert.encoder.layer.5.attention.self.query.bias', 'Qformer.bert.encoder.layer.5.attention.self.key.weight', 'Qformer.bert.encoder.layer.5.attention.self.key.bias', 'Qformer.bert.encoder.layer.5.attention.self.value.weight', 'Qformer.bert.encoder.layer.5.attention.self.value.bias', 'Qformer.bert.encoder.layer.5.attention.output.dense.weight', 'Qformer.bert.encoder.layer.5.attention.output.dense.bias', 'Qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.5.intermediate.dense.weight', 'Qformer.bert.encoder.layer.5.intermediate.dense.bias', 'Qformer.bert.encoder.layer.5.output.dense.weight', 'Qformer.bert.encoder.layer.5.output.dense.bias', 'Qformer.bert.encoder.layer.5.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.5.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.5.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.5.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.5.output_query.dense.weight', 'Qformer.bert.encoder.layer.5.output_query.dense.bias', 'Qformer.bert.encoder.layer.5.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.5.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.attention.self.query.weight', 'Qformer.bert.encoder.layer.6.attention.self.query.bias', 'Qformer.bert.encoder.layer.6.attention.self.key.weight', 'Qformer.bert.encoder.layer.6.attention.self.key.bias', 'Qformer.bert.encoder.layer.6.attention.self.value.weight', 'Qformer.bert.encoder.layer.6.attention.self.value.bias', 'Qformer.bert.encoder.layer.6.attention.output.dense.weight', 'Qformer.bert.encoder.layer.6.attention.output.dense.bias', 'Qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.6.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.6.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.6.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.6.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.6.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.6.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.6.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.intermediate.dense.weight', 'Qformer.bert.encoder.layer.6.intermediate.dense.bias', 'Qformer.bert.encoder.layer.6.output.dense.weight', 'Qformer.bert.encoder.layer.6.output.dense.bias', 'Qformer.bert.encoder.layer.6.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.6.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.6.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.6.output_query.dense.weight', 'Qformer.bert.encoder.layer.6.output_query.dense.bias', 'Qformer.bert.encoder.layer.6.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.6.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.7.attention.self.query.weight', 'Qformer.bert.encoder.layer.7.attention.self.query.bias', 'Qformer.bert.encoder.layer.7.attention.self.key.weight', 'Qformer.bert.encoder.layer.7.attention.self.key.bias', 'Qformer.bert.encoder.layer.7.attention.self.value.weight', 'Qformer.bert.encoder.layer.7.attention.self.value.bias', 'Qformer.bert.encoder.layer.7.attention.output.dense.weight', 'Qformer.bert.encoder.layer.7.attention.output.dense.bias', 'Qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.7.intermediate.dense.weight', 'Qformer.bert.encoder.layer.7.intermediate.dense.bias', 'Qformer.bert.encoder.layer.7.output.dense.weight', 'Qformer.bert.encoder.layer.7.output.dense.bias', 'Qformer.bert.encoder.layer.7.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.7.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.7.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.7.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.7.output_query.dense.weight', 'Qformer.bert.encoder.layer.7.output_query.dense.bias', 'Qformer.bert.encoder.layer.7.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.7.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.attention.self.query.weight', 'Qformer.bert.encoder.layer.8.attention.self.query.bias', 'Qformer.bert.encoder.layer.8.attention.self.key.weight', 'Qformer.bert.encoder.layer.8.attention.self.key.bias', 'Qformer.bert.encoder.layer.8.attention.self.value.weight', 'Qformer.bert.encoder.layer.8.attention.self.value.bias', 'Qformer.bert.encoder.layer.8.attention.output.dense.weight', 'Qformer.bert.encoder.layer.8.attention.output.dense.bias', 'Qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.8.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.8.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.8.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.8.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.8.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.8.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.8.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.intermediate.dense.weight', 'Qformer.bert.encoder.layer.8.intermediate.dense.bias', 'Qformer.bert.encoder.layer.8.output.dense.weight', 'Qformer.bert.encoder.layer.8.output.dense.bias', 'Qformer.bert.encoder.layer.8.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.8.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.8.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.8.output_query.dense.weight', 'Qformer.bert.encoder.layer.8.output_query.dense.bias', 'Qformer.bert.encoder.layer.8.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.8.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.9.attention.self.query.weight', 'Qformer.bert.encoder.layer.9.attention.self.query.bias', 'Qformer.bert.encoder.layer.9.attention.self.key.weight', 'Qformer.bert.encoder.layer.9.attention.self.key.bias', 'Qformer.bert.encoder.layer.9.attention.self.value.weight', 'Qformer.bert.encoder.layer.9.attention.self.value.bias', 'Qformer.bert.encoder.layer.9.attention.output.dense.weight', 'Qformer.bert.encoder.layer.9.attention.output.dense.bias', 'Qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.9.intermediate.dense.weight', 'Qformer.bert.encoder.layer.9.intermediate.dense.bias', 'Qformer.bert.encoder.layer.9.output.dense.weight', 'Qformer.bert.encoder.layer.9.output.dense.bias', 'Qformer.bert.encoder.layer.9.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.9.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.9.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.9.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.9.output_query.dense.weight', 'Qformer.bert.encoder.layer.9.output_query.dense.bias', 'Qformer.bert.encoder.layer.9.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.9.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.attention.self.query.weight', 'Qformer.bert.encoder.layer.10.attention.self.query.bias', 'Qformer.bert.encoder.layer.10.attention.self.key.weight', 'Qformer.bert.encoder.layer.10.attention.self.key.bias', 'Qformer.bert.encoder.layer.10.attention.self.value.weight', 'Qformer.bert.encoder.layer.10.attention.self.value.bias', 'Qformer.bert.encoder.layer.10.attention.output.dense.weight', 'Qformer.bert.encoder.layer.10.attention.output.dense.bias', 'Qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.crossattention.self.query.weight', 'Qformer.bert.encoder.layer.10.crossattention.self.query.bias', 'Qformer.bert.encoder.layer.10.crossattention.self.key.weight', 'Qformer.bert.encoder.layer.10.crossattention.self.key.bias', 'Qformer.bert.encoder.layer.10.crossattention.self.value.weight', 'Qformer.bert.encoder.layer.10.crossattention.self.value.bias', 'Qformer.bert.encoder.layer.10.crossattention.output.dense.weight', 'Qformer.bert.encoder.layer.10.crossattention.output.dense.bias', 'Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.intermediate.dense.weight', 'Qformer.bert.encoder.layer.10.intermediate.dense.bias', 'Qformer.bert.encoder.layer.10.output.dense.weight', 'Qformer.bert.encoder.layer.10.output.dense.bias', 'Qformer.bert.encoder.layer.10.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.10.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.10.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.10.output_query.dense.weight', 'Qformer.bert.encoder.layer.10.output_query.dense.bias', 'Qformer.bert.encoder.layer.10.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.10.output_query.LayerNorm.bias', 'Qformer.bert.encoder.layer.11.attention.self.query.weight', 'Qformer.bert.encoder.layer.11.attention.self.query.bias', 'Qformer.bert.encoder.layer.11.attention.self.key.weight', 'Qformer.bert.encoder.layer.11.attention.self.key.bias', 'Qformer.bert.encoder.layer.11.attention.self.value.weight', 'Qformer.bert.encoder.layer.11.attention.self.value.bias', 'Qformer.bert.encoder.layer.11.attention.output.dense.weight', 'Qformer.bert.encoder.layer.11.attention.output.dense.bias', 'Qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.11.intermediate.dense.weight', 'Qformer.bert.encoder.layer.11.intermediate.dense.bias', 'Qformer.bert.encoder.layer.11.output.dense.weight', 'Qformer.bert.encoder.layer.11.output.dense.bias', 'Qformer.bert.encoder.layer.11.output.LayerNorm.weight', 'Qformer.bert.encoder.layer.11.output.LayerNorm.bias', 'Qformer.bert.encoder.layer.11.intermediate_query.dense.weight', 'Qformer.bert.encoder.layer.11.intermediate_query.dense.bias', 'Qformer.bert.encoder.layer.11.output_query.dense.weight', 'Qformer.bert.encoder.layer.11.output_query.dense.bias', 'Qformer.bert.encoder.layer.11.output_query.LayerNorm.weight', 'Qformer.bert.encoder.layer.11.output_query.LayerNorm.bias', 'llm_model.model.embed_tokens.weight', 'llm_model.model.layers.0.self_attn.q_proj.weight', 'llm_model.model.layers.0.self_attn.k_proj.weight', 'llm_model.model.layers.0.self_attn.v_proj.weight', 'llm_model.model.layers.0.self_attn.o_proj.weight', 'llm_model.model.layers.0.mlp.gate_proj.weight', 'llm_model.model.layers.0.mlp.down_proj.weight', 'llm_model.model.layers.0.mlp.up_proj.weight', 'llm_model.model.layers.0.input_layernorm.weight', 'llm_model.model.layers.0.post_attention_layernorm.weight', 'llm_model.model.layers.1.self_attn.q_proj.weight', 'llm_model.model.layers.1.self_attn.k_proj.weight', 'llm_model.model.layers.1.self_attn.v_proj.weight', 'llm_model.model.layers.1.self_attn.o_proj.weight', 'llm_model.model.layers.1.mlp.gate_proj.weight', 'llm_model.model.layers.1.mlp.down_proj.weight', 'llm_model.model.layers.1.mlp.up_proj.weight', 'llm_model.model.layers.1.input_layernorm.weight', 'llm_model.model.layers.1.post_attention_layernorm.weight', 'llm_model.model.layers.2.self_attn.q_proj.weight', 'llm_model.model.layers.2.self_attn.k_proj.weight', 'llm_model.model.layers.2.self_attn.v_proj.weight', 'llm_model.model.layers.2.self_attn.o_proj.weight', 'llm_model.model.layers.2.mlp.gate_proj.weight', 'llm_model.model.layers.2.mlp.down_proj.weight', 'llm_model.model.layers.2.mlp.up_proj.weight', 'llm_model.model.layers.2.input_layernorm.weight', 'llm_model.model.layers.2.post_attention_layernorm.weight', 'llm_model.model.layers.3.self_attn.q_proj.weight', 'llm_model.model.layers.3.self_attn.k_proj.weight', 'llm_model.model.layers.3.self_attn.v_proj.weight', 'llm_model.model.layers.3.self_attn.o_proj.weight', 'llm_model.model.layers.3.mlp.gate_proj.weight', 'llm_model.model.layers.3.mlp.down_proj.weight', 'llm_model.model.layers.3.mlp.up_proj.weight', 'llm_model.model.layers.3.input_layernorm.weight', 'llm_model.model.layers.3.post_attention_layernorm.weight', 'llm_model.model.layers.4.self_attn.q_proj.weight', 'llm_model.model.layers.4.self_attn.k_proj.weight', 'llm_model.model.layers.4.self_attn.v_proj.weight', 'llm_model.model.layers.4.self_attn.o_proj.weight', 'llm_model.model.layers.4.mlp.gate_proj.weight', 'llm_model.model.layers.4.mlp.down_proj.weight', 'llm_model.model.layers.4.mlp.up_proj.weight', 'llm_model.model.layers.4.input_layernorm.weight', 'llm_model.model.layers.4.post_attention_layernorm.weight', 'llm_model.model.layers.5.self_attn.q_proj.weight', 'llm_model.model.layers.5.self_attn.k_proj.weight', 'llm_model.model.layers.5.self_attn.v_proj.weight', 'llm_model.model.layers.5.self_attn.o_proj.weight', 'llm_model.model.layers.5.mlp.gate_proj.weight', 'llm_model.model.layers.5.mlp.down_proj.weight', 'llm_model.model.layers.5.mlp.up_proj.weight', 'llm_model.model.layers.5.input_layernorm.weight', 'llm_model.model.layers.5.post_attention_layernorm.weight', 'llm_model.model.layers.6.self_attn.q_proj.weight', 'llm_model.model.layers.6.self_attn.k_proj.weight', 'llm_model.model.layers.6.self_attn.v_proj.weight', 'llm_model.model.layers.6.self_attn.o_proj.weight', 'llm_model.model.layers.6.mlp.gate_proj.weight', 'llm_model.model.layers.6.mlp.down_proj.weight', 'llm_model.model.layers.6.mlp.up_proj.weight', 'llm_model.model.layers.6.input_layernorm.weight', 'llm_model.model.layers.6.post_attention_layernorm.weight', 'llm_model.model.layers.7.self_attn.q_proj.weight', 'llm_model.model.layers.7.self_attn.k_proj.weight', 'llm_model.model.layers.7.self_attn.v_proj.weight', 'llm_model.model.layers.7.self_attn.o_proj.weight', 'llm_model.model.layers.7.mlp.gate_proj.weight', 'llm_model.model.layers.7.mlp.down_proj.weight', 'llm_model.model.layers.7.mlp.up_proj.weight', 'llm_model.model.layers.7.input_layernorm.weight', 'llm_model.model.layers.7.post_attention_layernorm.weight', 'llm_model.model.layers.8.self_attn.q_proj.weight', 'llm_model.model.layers.8.self_attn.k_proj.weight', 'llm_model.model.layers.8.self_attn.v_proj.weight', 'llm_model.model.layers.8.self_attn.o_proj.weight', 'llm_model.model.layers.8.mlp.gate_proj.weight', 'llm_model.model.layers.8.mlp.down_proj.weight', 'llm_model.model.layers.8.mlp.up_proj.weight', 'llm_model.model.layers.8.input_layernorm.weight', 'llm_model.model.layers.8.post_attention_layernorm.weight', 'llm_model.model.layers.9.self_attn.q_proj.weight', 'llm_model.model.layers.9.self_attn.k_proj.weight', 'llm_model.model.layers.9.self_attn.v_proj.weight', 'llm_model.model.layers.9.self_attn.o_proj.weight', 'llm_model.model.layers.9.mlp.gate_proj.weight', 'llm_model.model.layers.9.mlp.down_proj.weight', 'llm_model.model.layers.9.mlp.up_proj.weight', 'llm_model.model.layers.9.input_layernorm.weight', 'llm_model.model.layers.9.post_attention_layernorm.weight', 'llm_model.model.layers.10.self_attn.q_proj.weight', 'llm_model.model.layers.10.self_attn.k_proj.weight', 'llm_model.model.layers.10.self_attn.v_proj.weight', 'llm_model.model.layers.10.self_attn.o_proj.weight', 'llm_model.model.layers.10.mlp.gate_proj.weight', 'llm_model.model.layers.10.mlp.down_proj.weight', 'llm_model.model.layers.10.mlp.up_proj.weight', 'llm_model.model.layers.10.input_layernorm.weight', 'llm_model.model.layers.10.post_attention_layernorm.weight', 'llm_model.model.layers.11.self_attn.q_proj.weight', 'llm_model.model.layers.11.self_attn.k_proj.weight', 'llm_model.model.layers.11.self_attn.v_proj.weight', 'llm_model.model.layers.11.self_attn.o_proj.weight', 'llm_model.model.layers.11.mlp.gate_proj.weight', 'llm_model.model.layers.11.mlp.down_proj.weight', 'llm_model.model.layers.11.mlp.up_proj.weight', 'llm_model.model.layers.11.input_layernorm.weight', 'llm_model.model.layers.11.post_attention_layernorm.weight', 'llm_model.model.layers.12.self_attn.q_proj.weight', 'llm_model.model.layers.12.self_attn.k_proj.weight', 'llm_model.model.layers.12.self_attn.v_proj.weight', 'llm_model.model.layers.12.self_attn.o_proj.weight', 'llm_model.model.layers.12.mlp.gate_proj.weight', 'llm_model.model.layers.12.mlp.down_proj.weight', 'llm_model.model.layers.12.mlp.up_proj.weight', 'llm_model.model.layers.12.input_layernorm.weight', 'llm_model.model.layers.12.post_attention_layernorm.weight', 'llm_model.model.layers.13.self_attn.q_proj.weight', 'llm_model.model.layers.13.self_attn.k_proj.weight', 'llm_model.model.layers.13.self_attn.v_proj.weight', 'llm_model.model.layers.13.self_attn.o_proj.weight', 'llm_model.model.layers.13.mlp.gate_proj.weight', 'llm_model.model.layers.13.mlp.down_proj.weight', 'llm_model.model.layers.13.mlp.up_proj.weight', 'llm_model.model.layers.13.input_layernorm.weight', 'llm_model.model.layers.13.post_attention_layernorm.weight', 'llm_model.model.layers.14.self_attn.q_proj.weight', 'llm_model.model.layers.14.self_attn.k_proj.weight', 'llm_model.model.layers.14.self_attn.v_proj.weight', 'llm_model.model.layers.14.self_attn.o_proj.weight', 'llm_model.model.layers.14.mlp.gate_proj.weight', 'llm_model.model.layers.14.mlp.down_proj.weight', 'llm_model.model.layers.14.mlp.up_proj.weight', 'llm_model.model.layers.14.input_layernorm.weight', 'llm_model.model.layers.14.post_attention_layernorm.weight', 'llm_model.model.layers.15.self_attn.q_proj.weight', 'llm_model.model.layers.15.self_attn.k_proj.weight', 'llm_model.model.layers.15.self_attn.v_proj.weight', 'llm_model.model.layers.15.self_attn.o_proj.weight', 'llm_model.model.layers.15.mlp.gate_proj.weight', 'llm_model.model.layers.15.mlp.down_proj.weight', 'llm_model.model.layers.15.mlp.up_proj.weight', 'llm_model.model.layers.15.input_layernorm.weight', 'llm_model.model.layers.15.post_attention_layernorm.weight', 'llm_model.model.layers.16.self_attn.q_proj.weight', 'llm_model.model.layers.16.self_attn.k_proj.weight', 'llm_model.model.layers.16.self_attn.v_proj.weight', 'llm_model.model.layers.16.self_attn.o_proj.weight', 'llm_model.model.layers.16.mlp.gate_proj.weight', 'llm_model.model.layers.16.mlp.down_proj.weight', 'llm_model.model.layers.16.mlp.up_proj.weight', 'llm_model.model.layers.16.input_layernorm.weight', 'llm_model.model.layers.16.post_attention_layernorm.weight', 'llm_model.model.layers.17.self_attn.q_proj.weight', 'llm_model.model.layers.17.self_attn.k_proj.weight', 'llm_model.model.layers.17.self_attn.v_proj.weight', 'llm_model.model.layers.17.self_attn.o_proj.weight', 'llm_model.model.layers.17.mlp.gate_proj.weight', 'llm_model.model.layers.17.mlp.down_proj.weight', 'llm_model.model.layers.17.mlp.up_proj.weight', 'llm_model.model.layers.17.input_layernorm.weight', 'llm_model.model.layers.17.post_attention_layernorm.weight', 'llm_model.model.layers.18.self_attn.q_proj.weight', 'llm_model.model.layers.18.self_attn.k_proj.weight', 'llm_model.model.layers.18.self_attn.v_proj.weight', 'llm_model.model.layers.18.self_attn.o_proj.weight', 'llm_model.model.layers.18.mlp.gate_proj.weight', 'llm_model.model.layers.18.mlp.down_proj.weight', 'llm_model.model.layers.18.mlp.up_proj.weight', 'llm_model.model.layers.18.input_layernorm.weight', 'llm_model.model.layers.18.post_attention_layernorm.weight', 'llm_model.model.layers.19.self_attn.q_proj.weight', 'llm_model.model.layers.19.self_attn.k_proj.weight', 'llm_model.model.layers.19.self_attn.v_proj.weight', 'llm_model.model.layers.19.self_attn.o_proj.weight', 'llm_model.model.layers.19.mlp.gate_proj.weight', 'llm_model.model.layers.19.mlp.down_proj.weight', 'llm_model.model.layers.19.mlp.up_proj.weight', 'llm_model.model.layers.19.input_layernorm.weight', 'llm_model.model.layers.19.post_attention_layernorm.weight', 'llm_model.model.layers.20.self_attn.q_proj.weight', 'llm_model.model.layers.20.self_attn.k_proj.weight', 'llm_model.model.layers.20.self_attn.v_proj.weight', 'llm_model.model.layers.20.self_attn.o_proj.weight', 'llm_model.model.layers.20.mlp.gate_proj.weight', 'llm_model.model.layers.20.mlp.down_proj.weight', 'llm_model.model.layers.20.mlp.up_proj.weight', 'llm_model.model.layers.20.input_layernorm.weight', 'llm_model.model.layers.20.post_attention_layernorm.weight', 'llm_model.model.layers.21.self_attn.q_proj.weight', 'llm_model.model.layers.21.self_attn.k_proj.weight', 'llm_model.model.layers.21.self_attn.v_proj.weight', 'llm_model.model.layers.21.self_attn.o_proj.weight', 'llm_model.model.layers.21.mlp.gate_proj.weight', 'llm_model.model.layers.21.mlp.down_proj.weight', 'llm_model.model.layers.21.mlp.up_proj.weight', 'llm_model.model.layers.21.input_layernorm.weight', 'llm_model.model.layers.21.post_attention_layernorm.weight', 'llm_model.model.layers.22.self_attn.q_proj.weight', 'llm_model.model.layers.22.self_attn.k_proj.weight', 'llm_model.model.layers.22.self_attn.v_proj.weight', 'llm_model.model.layers.22.self_attn.o_proj.weight', 'llm_model.model.layers.22.mlp.gate_proj.weight', 'llm_model.model.layers.22.mlp.down_proj.weight', 'llm_model.model.layers.22.mlp.up_proj.weight', 'llm_model.model.layers.22.input_layernorm.weight', 'llm_model.model.layers.22.post_attention_layernorm.weight', 'llm_model.model.layers.23.self_attn.q_proj.weight', 'llm_model.model.layers.23.self_attn.k_proj.weight', 'llm_model.model.layers.23.self_attn.v_proj.weight', 'llm_model.model.layers.23.self_attn.o_proj.weight', 'llm_model.model.layers.23.mlp.gate_proj.weight', 'llm_model.model.layers.23.mlp.down_proj.weight', 'llm_model.model.layers.23.mlp.up_proj.weight', 'llm_model.model.layers.23.input_layernorm.weight', 'llm_model.model.layers.23.post_attention_layernorm.weight', 'llm_model.model.layers.24.self_attn.q_proj.weight', 'llm_model.model.layers.24.self_attn.k_proj.weight', 'llm_model.model.layers.24.self_attn.v_proj.weight', 'llm_model.model.layers.24.self_attn.o_proj.weight', 'llm_model.model.layers.24.mlp.gate_proj.weight', 'llm_model.model.layers.24.mlp.down_proj.weight', 'llm_model.model.layers.24.mlp.up_proj.weight', 'llm_model.model.layers.24.input_layernorm.weight', 'llm_model.model.layers.24.post_attention_layernorm.weight', 'llm_model.model.layers.25.self_attn.q_proj.weight', 'llm_model.model.layers.25.self_attn.k_proj.weight', 'llm_model.model.layers.25.self_attn.v_proj.weight', 'llm_model.model.layers.25.self_attn.o_proj.weight', 'llm_model.model.layers.25.mlp.gate_proj.weight', 'llm_model.model.layers.25.mlp.down_proj.weight', 'llm_model.model.layers.25.mlp.up_proj.weight', 'llm_model.model.layers.25.input_layernorm.weight', 'llm_model.model.layers.25.post_attention_layernorm.weight', 'llm_model.model.layers.26.self_attn.q_proj.weight', 'llm_model.model.layers.26.self_attn.k_proj.weight', 'llm_model.model.layers.26.self_attn.v_proj.weight', 'llm_model.model.layers.26.self_attn.o_proj.weight', 'llm_model.model.layers.26.mlp.gate_proj.weight', 'llm_model.model.layers.26.mlp.down_proj.weight', 'llm_model.model.layers.26.mlp.up_proj.weight', 'llm_model.model.layers.26.input_layernorm.weight', 'llm_model.model.layers.26.post_attention_layernorm.weight', 'llm_model.model.layers.27.self_attn.q_proj.weight', 'llm_model.model.layers.27.self_attn.k_proj.weight', 'llm_model.model.layers.27.self_attn.v_proj.weight', 'llm_model.model.layers.27.self_attn.o_proj.weight', 'llm_model.model.layers.27.mlp.gate_proj.weight', 'llm_model.model.layers.27.mlp.down_proj.weight', 'llm_model.model.layers.27.mlp.up_proj.weight', 'llm_model.model.layers.27.input_layernorm.weight', 'llm_model.model.layers.27.post_attention_layernorm.weight', 'llm_model.model.layers.28.self_attn.q_proj.weight', 'llm_model.model.layers.28.self_attn.k_proj.weight', 'llm_model.model.layers.28.self_attn.v_proj.weight', 'llm_model.model.layers.28.self_attn.o_proj.weight', 'llm_model.model.layers.28.mlp.gate_proj.weight', 'llm_model.model.layers.28.mlp.down_proj.weight', 'llm_model.model.layers.28.mlp.up_proj.weight', 'llm_model.model.layers.28.input_layernorm.weight', 'llm_model.model.layers.28.post_attention_layernorm.weight', 'llm_model.model.layers.29.self_attn.q_proj.weight', 'llm_model.model.layers.29.self_attn.k_proj.weight', 'llm_model.model.layers.29.self_attn.v_proj.weight', 'llm_model.model.layers.29.self_attn.o_proj.weight', 'llm_model.model.layers.29.mlp.gate_proj.weight', 'llm_model.model.layers.29.mlp.down_proj.weight', 'llm_model.model.layers.29.mlp.up_proj.weight', 'llm_model.model.layers.29.input_layernorm.weight', 'llm_model.model.layers.29.post_attention_layernorm.weight', 'llm_model.model.layers.30.self_attn.q_proj.weight', 'llm_model.model.layers.30.self_attn.k_proj.weight', 'llm_model.model.layers.30.self_attn.v_proj.weight', 'llm_model.model.layers.30.self_attn.o_proj.weight', 'llm_model.model.layers.30.mlp.gate_proj.weight', 'llm_model.model.layers.30.mlp.down_proj.weight', 'llm_model.model.layers.30.mlp.up_proj.weight', 'llm_model.model.layers.30.input_layernorm.weight', 'llm_model.model.layers.30.post_attention_layernorm.weight', 'llm_model.model.layers.31.self_attn.q_proj.weight', 'llm_model.model.layers.31.self_attn.k_proj.weight', 'llm_model.model.layers.31.self_attn.v_proj.weight', 'llm_model.model.layers.31.self_attn.o_proj.weight', 'llm_model.model.layers.31.mlp.gate_proj.weight', 'llm_model.model.layers.31.mlp.down_proj.weight', 'llm_model.model.layers.31.mlp.up_proj.weight', 'llm_model.model.layers.31.input_layernorm.weight', 'llm_model.model.layers.31.post_attention_layernorm.weight', 'llm_model.model.norm.weight', 'llm_model.lm_head.weight', 'llm_proj.weight', 'llm_proj.bias']\n",
      "load checkpoint from /home/iammingggg/detection/LAVIS/checkpoints/COCO_90k_SD2_SD2IP.pth\n",
      "Qformer.bert.embeddings.train_word_embeddings.weight True\n",
      "llm_model.model.train_embed_tokens.weight True\n",
      "Load model OK!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct_textinv\", model_type=\"vicuna7b\", is_eval=True, device=device)\n",
    "\n",
    "print(f'Load model OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Log path: /home/denny/LAVIS/deepfake-detection/log/log.txt\n",
      "Q1: Is this photo real?\n",
      "Found 100 images with label \"yes\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "     Acc: 98.00%\n",
      "\n",
      "Found 100 images with label \"yes\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "     Acc: 98.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logPath = '/home/denny/LAVIS/deepfake-detection/log/log.txt'\n",
    "# logPath = '/home/denny/LAVIS/deepfake-detection/log/log2.txt'\n",
    "# logPath = '/home/denny/LAVIS/deepfake-detection/log/SD2_SD2IP_90k_lora_onlyCommon.txt'\n",
    "# logPath = '/home/denny/LAVIS/deepfake-detection/log/SD2_SD2IP_90k_lora_onlyCommon2.txt'\n",
    "\n",
    "q1 = \"Is this photo real?\"\n",
    "q2 = \"Is this photo real [*]?\"\n",
    "\n",
    "file = open(logPath, 'a')\n",
    "file.close()\n",
    "\n",
    "instruct = InstructBLIP()\n",
    "instruct.LoadModels(model, vis_processors, txt_processors, device)\n",
    "\n",
    "print(f'Log path: {logPath}')\n",
    "print(f'Q1: {q1}')\n",
    "# print(f'Q2: {q2}')\n",
    "\n",
    "# csvfiles = [\n",
    "# # \"/eva_data0/denny/textual_inversion/debug_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_SD2_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_SDXL_label.csv\", \n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_IF_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_DALLE_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_SGXL_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Control_COCO_label.csv\",\n",
    "# # \"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_lama_label.csv\",\n",
    "# # \"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_SD2IP_label.csv\",\n",
    "# # \"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_lte_label.csv\",\n",
    "# # \"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_SD2SR_label.csv\",\n",
    "# # \"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_deeperforensics_faceOnly_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_AdvAtk_Imagenet_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Backdoor_Imagenet_label.csv\",\n",
    "# # \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_DataPoison_Imagenet_label.csv\",\n",
    "# ]\n",
    "\n",
    "roots_and_labels = [\n",
    "    [\"/eva_data0/denny/textual_inversion/debug/0_real/\", \"yes\"],\n",
    "    [\"/eva_data0/denny/textual_inversion/debug/0_real/\", \"yes\"],\n",
    "]\n",
    "\n",
    "for root, label in roots_and_labels:\n",
    "    instruct.LoadData(roots=root, labels=label)\n",
    "\n",
    "    question = q1\n",
    "    acc, confusion_mat, pretrained_ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "    print(f'Question: {question}')\n",
    "    print(f'     Acc: {acc*100:.2f}%\\n')\n",
    "\n",
    "    # question = q2\n",
    "    # acc, confusion_mat, finetuned_ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "    # print(f'Question: {question}')\n",
    "    # print(f'Acc: {acc*100:.2f}%')\n",
    "\n",
    "    # comb_acc, comb_confusion_mat, comb_ans = print_combine_result(pretrained_ans_list, finetuned_ans_list, labels, logPath=logPath)\n",
    "    # print(f'[Combination]')\n",
    "    # print(f'Acc: {comb_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fake']\n"
     ]
    }
   ],
   "source": [
    "path = \"/eva_data0/denny/textual_inversion/debug/1_fake/common/00009.png\"\n",
    "image = Image.open(path)\n",
    "ans = instruct.Query(image, q1)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextInvDataset(Dataset):\n",
    "    def __init__(self, csv, vis_processors=None, txt_processors=None):\n",
    "        \n",
    "        self.path_and_labels = pd.read_csv(csv, index_col=\"img_path\")\n",
    "        self.vis_processors = vis_processors\n",
    "        self.txt_processors = txt_processors\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(list(self.path_and_labels.index))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image_path = list(self.path_and_labels.index)[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.vis_processors:\n",
    "            image = self.vis_processors(image)\n",
    "        \n",
    "        label = self.path_and_labels.loc[image_path, \"label\"]\n",
    "        \n",
    "        is_uncommon = \"uncommon\" in image_path\n",
    "\n",
    "        return image, label, is_uncommon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is for query lost of images\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "class InstructBLIP():\n",
    "    def __init__(self, name=\"blip2_vicuna_instruct_textinv\", model_type=\"vicuna7b\", is_eval=True, device=\"cpu\") -> None:\n",
    "        print(f'Loading model...')\n",
    "        #self.model, self.vis_processors, self.txt_processors = load_model_and_preprocess(name, model_type, is_eval, device)\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # QA\n",
    "        self.question = \"\"\n",
    "        \n",
    "        # results\n",
    "        self.acc = None\n",
    "        self.confusion_mat = None\n",
    "        \n",
    "        self.acc_3class = None\n",
    "        self.confusion_mat_3class = None\n",
    "        \n",
    "        self.com_acc = None\n",
    "        self.com_confusion_mat = None\n",
    "        self.uncom_acc = None\n",
    "        self.uncom_confusion_mat = None\n",
    "\n",
    "    def LoadModels(self, model, vis_processors, txt_processors, device):\n",
    "        self.model = model\n",
    "        self.vis_processors = vis_processors\n",
    "        self.txt_processors = txt_processors\n",
    "        self.device = device\n",
    "    \n",
    "    def LoadImages(self, dir, num):\n",
    "        onlyfiles = []\n",
    "        \n",
    "        for f in sorted(listdir(dir)):\n",
    "            if isfile(join(dir, f)):\n",
    "                onlyfiles.append(join(dir, f))\n",
    "        \n",
    "        onlyfiles = random.sample(onlyfiles, num)\n",
    "        \n",
    "        raw_img_list = []\n",
    "        with tqdm(total=len(onlyfiles), desc=f'Loading imgs from {dir}') as pbar:\n",
    "            for f in onlyfiles:\n",
    "                raw_img = Image.open(f).convert(\"RGB\")\n",
    "                raw_img_list.append(raw_img)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        return raw_img_list\n",
    "\n",
    "    def LoadData(self, real_dir, fake_dir, num=1000):\n",
    "        #real_imgs = LoadImages(join(root_dir, \"0_real\"))\n",
    "        #fake_imgs = LoadImages(join(root_dir, \"1_fake\"))\n",
    "        real_imgs = self.LoadImages(real_dir, num)\n",
    "        fake_imgs = self.LoadImages(fake_dir, num)\n",
    "        \n",
    "        self.imgs = real_imgs + fake_imgs\n",
    "        self.labels = [0]*len(real_imgs) + [1]*len(fake_imgs)\n",
    "        #return self.imgs, self.labels\n",
    "      \n",
    "    def LoadData_batch(self, csv_path):\n",
    "        self.csv = csv_path\n",
    "        self.dataset = TextInvDataset(csv=csv_path, vis_processors=self.vis_processors[\"eval\"])\n",
    "        self.dataloader = DataLoader(dataset=self.dataset, batch_size=8, shuffle=False, num_workers=8)    \n",
    "        \n",
    "    def LoadData3Class(self, real_dir, fake_common_dir, fake_uncommon_dir, num=[1000, 500, 500]):\n",
    "        #real_imgs = LoadImages(join(root_dir, \"0_real\"))\n",
    "        #fake_imgs = LoadImages(join(root_dir, \"1_fake\"))\n",
    "        self.num = num\n",
    "        real_imgs = self.LoadImages(real_dir, num[0])\n",
    "        fake_common_imgs = self.LoadImages(fake_common_dir, num[1])\n",
    "        fake_uncommon_imgs = self.LoadImages(fake_uncommon_dir, num[2])\n",
    "        \n",
    "        self.imgs = real_imgs + fake_common_imgs + fake_uncommon_imgs\n",
    "        self.labels = [0]*len(real_imgs) + [1]*(len(fake_common_imgs)+len(fake_uncommon_imgs))\n",
    "        self.label_3class = [0]*len(real_imgs) + [1]*len(fake_common_imgs) + [2]*len(fake_uncommon_imgs)\n",
    "        #return self.imgs, self.labels, self.label_3class\n",
    "\n",
    "    def QueryImgs(self, question, true_string=\"yes\"):\n",
    "        self.ans_list = []\n",
    "        self.question = question\n",
    "        \n",
    "        with tqdm(total=len(self.imgs), desc=f'Answering') as pbar:\n",
    "            for idx, img in enumerate(self.imgs):\n",
    "                image = self.vis_processors[\"eval\"](img).unsqueeze(0).to(self.device)\n",
    "\n",
    "                samples = {\"image\": image, \"text_input\": question}\n",
    "                \n",
    "                ans = self.model.predict_answers(samples=samples, inference_method=\"generate\")[0]\n",
    "                self.ans_list.append(0 if ans == true_string else 1)\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        self.acc = accuracy_score(self.labels, self.ans_list)\n",
    "        self.confusion_mat = confusion_matrix(self.labels, self.ans_list)\n",
    "        \n",
    "        self.PrintResult()\n",
    "        \n",
    "        return self.acc, self.confusion_mat, self.ans_list\n",
    "    \n",
    "    def QueryImgs_batch(self, question, true_string=\"yes\", logPath='log.txt'):\n",
    "        self.labels = []\n",
    "        self.label_3class = []\n",
    "        self.ans_list = []\n",
    "        self.question = question\n",
    "        \n",
    "        for image, label, is_uncommon in tqdm(self.dataloader):\n",
    "            \n",
    "            image = image.to(self.device)\n",
    "            \n",
    "            questions = [self.question] * image.shape[0]\n",
    "            samples = {\"image\": image, \"text_input\": questions}\n",
    "\n",
    "            ans = self.model.predict_answers(samples=samples, inference_method=\"generate\")\n",
    "            pred_label = [0 if a == true_string else 1 for a in ans]\n",
    "            self.ans_list += pred_label\n",
    "            \n",
    "            label = [0 if l == true_string else 1 for l in label]\n",
    "            self.labels += label\n",
    "            \n",
    "            label_3class = label.copy()\n",
    "            label_3class = [2 if is_uncommon[idx] else l for idx, l in enumerate(label)]\n",
    "            \n",
    "            self.label_3class += label_3class\n",
    "        \n",
    "        self.acc = accuracy_score(self.labels, self.ans_list)\n",
    "        self.confusion_mat = confusion_matrix(self.labels, self.ans_list)\n",
    "        \n",
    "        self.ans_list = np.array(self.ans_list)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.label_3class = np.array(self.label_3class)\n",
    "        \n",
    "        self.PrintResult(three_class=True, logPath=logPath)\n",
    "        \n",
    "        return self.acc, self.confusion_mat, self.ans_list, self.labels, self.label_3class\n",
    "    \n",
    "    def Query(self, image, question):\n",
    "        image = self.vis_processors[\"eval\"](image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        samples = {\"image\": image, \"text_input\": question}\n",
    "        ans = self.model.predict_answers(samples=samples, inference_method=\"generate\")[0]\n",
    "        return ans\n",
    "\n",
    "    def PrintResult(self, three_class=False, acc=None, confusion_mat=None, ans_list=None, labels=None, label_3class=None, logPath=None):\n",
    "        \n",
    "        if acc:\n",
    "            self.acc = acc\n",
    "        if confusion_mat:\n",
    "            self.confusion_mat = confusion_mat\n",
    "        if ans_list:\n",
    "            self.ans_list = ans_list\n",
    "        if labels:\n",
    "            self.labels = labels\n",
    "        if label_3class:\n",
    "            self.label_3class = label_3class\n",
    "        \n",
    "        if logPath:\n",
    "            logfile = open(logPath, 'a')\n",
    "        \n",
    "        if three_class:\n",
    "            #assert type(self.num) == list, \"Type of num should be list.\"\n",
    "            \n",
    "            print(f'[TIME]      : {time.ctime()}', file=logfile)\n",
    "            print(f'[Finetuned] : {self.model.finetuned}', file=logfile)\n",
    "            print(f'[Data csv]  : {self.csv}', file=logfile)\n",
    "            print(f'[Question]  : {self.question}\\n', file=logfile)\n",
    "            \n",
    "            print(f'=== Overall ===', file=logfile)\n",
    "            print(f'Acc: {self.acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "            \n",
    "            real_ans_list = self.ans_list[self.label_3class==0]\n",
    "            real_label = [0] * len(real_ans_list)\n",
    "            self.real_acc = accuracy_score(real_label, real_ans_list)\n",
    "            self.real_confusion_mat = confusion_matrix(real_label, real_ans_list, labels=[0,1])\n",
    "            print(f'=== Real images ===', file=logfile)\n",
    "            print(f'Acc: {self.real_acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.real_confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "            \n",
    "            com_ans_list = self.ans_list[self.label_3class==1]\n",
    "            com_label = [1] * len(com_ans_list)\n",
    "            self.com_acc = accuracy_score(com_label, com_ans_list)\n",
    "            self.com_confusion_mat = confusion_matrix(com_label, com_ans_list, labels=[0,1])\n",
    "            print(f'=== Common fake images ===', file=logfile)\n",
    "            print(f'Acc: {self.com_acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.com_confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "            \n",
    "            uncom_ans_list = self.ans_list[self.label_3class==2]\n",
    "            uncom_label = [1] * len(uncom_ans_list)\n",
    "            self.uncom_acc = accuracy_score(uncom_label, uncom_ans_list)\n",
    "            self.uncom_confusion_mat = confusion_matrix(uncom_label, uncom_ans_list, labels=[0,1])\n",
    "            print(f'=== Uncommon fake images ===', file=logfile)\n",
    "            print(f'Acc: {self.uncom_acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.uncom_confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "        else:\n",
    "            print(f'Question: {self.question}\\n', file=logfile)\n",
    "            print(f'Acc: {self.acc*100:.2f}%', file=logfile)\n",
    "            self.PrintConfusion(self.confusion_mat, logfile=logfile)\n",
    "            print('\\n', file=logfile)\n",
    "        \n",
    "        logfile.close()\n",
    "    \n",
    "    def PrintConfusion(self, mat, logfile):\n",
    "        padding = ' '\n",
    "        print(f'        | Pred real | Pred fake |', file=logfile)\n",
    "        print(f'GT real | {mat[0, 0]:{padding}<{10}}| {mat[0, 1]:{padding}<{11}}|', file=logfile)\n",
    "        print(f'GT fake | {mat[1, 0]:{padding}<{10}}| {mat[1, 1]:{padding}<{11}}|', file=logfile)\n",
    "        \n",
    "    def MultipleAns(self, ans1, ans2):\n",
    "    \n",
    "        # Q1: Is this photo common in real world?\n",
    "        # Q2: Is this photo generated by a model?\n",
    "        \n",
    "        final_ans = []\n",
    "        for ans in zip(ans1, ans2):\n",
    "            if ans[0] == 0 and ans[1] == 0:\n",
    "                final_ans.append(0)\n",
    "            else:\n",
    "                final_ans.append(1)\n",
    "        \n",
    "        acc = accuracy_score(self.labels, final_ans)\n",
    "        confusion_mat = confusion_matrix(self.labels, final_ans)\n",
    "        print(f'Accuracy: {acc*100:.2f}%')\n",
    "        self.PrintConfusion(confusion_mat)\n",
    "        \n",
    "        self.ans_list = final_ans\n",
    "        self.acc = acc\n",
    "        self.confusion_mat = confusion_mat\n",
    "        \n",
    "        return acc, confusion_mat, final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logPath = '/home/denny/LAVIS/deepfake-detection/log/log.txt'\n",
    "# logPath = '/home/denny/LAVIS/deepfake-detection/log/SD2_SD2IP_90k_lora_onlyCommon.txt'\n",
    "logPath = '/home/denny/LAVIS/deepfake-detection/log/SD2_SD2IP_90k_lora_onlyCommon2.txt'\n",
    "\n",
    "q1 = \"Is this photo real?\"\n",
    "# q1 = \"Is this photo fake?\"\n",
    "# q2 = \"Is this photo real?\"\n",
    "\n",
    "file = open(logPath, 'a')\n",
    "file.close()\n",
    "\n",
    "instruct = InstructBLIP()\n",
    "instruct.LoadModels(model, vis_processors, txt_processors, device)\n",
    "\n",
    "print(f'Log path: {logPath}')\n",
    "print(f'Q1: {q1}')\n",
    "# print(f'Q2: {q2}')\n",
    "\n",
    "csvfiles = [\n",
    "# \"/eva_data0/denny/textual_inversion/debug_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_SD2_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_SDXL_label.csv\", \n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_IF_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_DALLE_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_SGXL_label.csv\",\n",
    "# \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Control_COCO_label.csv\",\n",
    "\"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_lama_label.csv\",\n",
    "\"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_SD2IP_label.csv\",\n",
    "\"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_lte_label.csv\",\n",
    "\"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_SD2SR_label.csv\",\n",
    "\"/eva_data0/iammingggg/textual_inversion/60k_6k_6k/test_deeperforensics_faceOnly_label.csv\",\n",
    "\"/eva_data0/denny/textual_inversion/60k_6k_6k/test_AdvAtk_Imagenet_label.csv\",\n",
    "\"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Backdoor_Imagenet_label.csv\",\n",
    "\"/eva_data0/denny/textual_inversion/60k_6k_6k/test_DataPoison_Imagenet_label.csv\",\n",
    "]\n",
    "\n",
    "for csv_path in csvfiles:\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "print(f'Load data from {csv_path}')\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, pretrained_ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "print(f'Question: {question}')\n",
    "print(f'Acc: {acc*100:.2f}%')\n",
    "\n",
    "# question = q2\n",
    "# acc, confusion_mat, finetuned_ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "# print(f'Question: {question}')\n",
    "# print(f'Acc: {acc*100:.2f}%')\n",
    "\n",
    "# comb_acc, comb_confusion_mat, comb_ans = print_combine_result(pretrained_ans_list, finetuned_ans_list, labels, logPath=logPath)\n",
    "# print(f'[Combination]')\n",
    "# print(f'Acc: {comb_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer OK!\n",
      "visual encoder OK!\n",
      "Q-former OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM OK!\n",
      "Added 1 tokens to Qformer tokenizer.\n",
      "Added 1 tokens to llm tokenizer.\n",
      "Qformer.bert.embeddings.train_word_embeddings.weight True\n",
      "llm_model.model.train_embed_tokens.weight True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "# model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct_textinv\", model_type=\"vicuna7b\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "instruct = InstructBLIP()\n",
    "instruct.LoadModels(model, vis_processors, txt_processors, device)\n",
    "\n",
    "# logPath = 'log/SDXL_postfix_onlyCommon.txt'\n",
    "logPath = 'log/log.txt'\n",
    "q1 = \"Is this photo real?\"\n",
    "q2 = \"Is this photo real [*]?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>instruct.LoadData_batch(csv_path=csv_path)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>question = q1                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>5 acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=q     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span>question = q2                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8 </span>acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=q     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">QueryImgs_batch</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">119</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>samples = {<span style=\"color: #808000; text-decoration-color: #808000\">\"image\"</span>: image, <span style=\"color: #808000; text-decoration-color: #808000\">\"text_input\"</span>: questions}                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>samplesForOut = {<span style=\"color: #808000; text-decoration-color: #808000\">\"image\"</span>: image, <span style=\"color: #808000; text-decoration-color: #808000\">\"text_input\"</span>: questions, <span style=\"color: #808000; text-decoration-color: #808000\">\"text_output\"</span>: lab   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>119 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(samplesForOut) <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># for debug</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>ans = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.predict_answers(samples=samples, inference_method=<span style=\"color: #808000; text-decoration-color: #808000\">\"generate</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>pred_label = [<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> a == true_string <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> a <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> ans]                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ans_list += pred_label                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/LAVIS/lavis/models/blip2_models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">blip2_vicuna_instruct_textinv.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">252</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">249 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">250 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>image = samples[<span style=\"color: #808000; text-decoration-color: #808000\">\"image\"</span>]                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">251 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.maybe_autocast():                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>252 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>image_embeds = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_vision(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.visual_encoder(image))                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>image_atts = torch.ones(image_embeds.size()[:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], dtype=torch.long).to(image.dev   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>bs = image.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/LAVIS/lavis/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eva_vit.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">350</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">347 #             return x[:, 0]</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">348 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">349 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x):                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>350 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.forward_features(x)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">351 #         x = self.head(x)</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">352 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">353 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/LAVIS/lavis/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eva_vit.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">339</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward_features</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.use_checkpoint:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>x = checkpoint.checkpoint(blk, x, rel_pos_bias)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">338 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>339 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>x = blk(x, rel_pos_bias)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">341 #         x = self.norm(x)</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">342 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/LAVIS/lavis/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eva_vit.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">175</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x, rel_pos_bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gamma_1 <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>175 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.drop_path(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm1(x), rel_pos_bias=rel_pos_bias))    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.drop_path(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm2(x)))                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">177 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">178 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.drop_path(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gamma_1 * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm1(x), rel_pos_bias=   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/denny/LAVIS/lavis/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eva_vit.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">124</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.q_bias <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>qkv_bias = torch.cat((<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.q_bias, torch.zeros_like(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.v_bias, requires_gr   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permut</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>qkv = F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>=x, weight=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.qkv.weight, bias=qkv_bias)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>qkv = qkv.reshape(B, N, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).permute(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>q, k, v = qkv[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], qkv[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], qkv[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># make torchscript happy (cannot use tensor a</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.70</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.79</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.69</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.35</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory\n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0minstruct.LoadData_batch(csv_path=csv_path)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mquestion = q1                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m5 acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=q     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0mquestion = q2                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m8 \u001b[0macc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=q     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mQueryImgs_batch\u001b[0m:\u001b[94m119\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   \u001b[0msamples = {\u001b[33m\"\u001b[0m\u001b[33mimage\u001b[0m\u001b[33m\"\u001b[0m: image, \u001b[33m\"\u001b[0m\u001b[33mtext_input\u001b[0m\u001b[33m\"\u001b[0m: questions}                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   │   \u001b[0msamplesForOut = {\u001b[33m\"\u001b[0m\u001b[33mimage\u001b[0m\u001b[33m\"\u001b[0m: image, \u001b[33m\"\u001b[0m\u001b[33mtext_input\u001b[0m\u001b[33m\"\u001b[0m: questions, \u001b[33m\"\u001b[0m\u001b[33mtext_output\u001b[0m\u001b[33m\"\u001b[0m: lab   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m119 \u001b[2m│   │   │   \u001b[0mout = \u001b[96mself\u001b[0m.model(samplesForOut) \u001b[2m# for debug\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0mans = \u001b[96mself\u001b[0m.model.predict_answers(samples=samples, inference_method=\u001b[33m\"\u001b[0m\u001b[33mgenerate\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   │   \u001b[0mpred_label = [\u001b[94m0\u001b[0m \u001b[94mif\u001b[0m a == true_string \u001b[94melse\u001b[0m \u001b[94m1\u001b[0m \u001b[94mfor\u001b[0m a \u001b[95min\u001b[0m ans]                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.ans_list += pred_label                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/LAVIS/lavis/models/blip2_models/\u001b[0m\u001b[1;33mblip2_vicuna_instruct_textinv.py\u001b[0m:\u001b[94m252\u001b[0m in \u001b[92mforward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   │   \u001b[0mimage = samples[\u001b[33m\"\u001b[0m\u001b[33mimage\u001b[0m\u001b[33m\"\u001b[0m]                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.maybe_autocast():                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m252 \u001b[2m│   │   │   \u001b[0mimage_embeds = \u001b[96mself\u001b[0m.ln_vision(\u001b[96mself\u001b[0m.visual_encoder(image))                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   │   \u001b[0mimage_atts = torch.ones(image_embeds.size()[:-\u001b[94m1\u001b[0m], dtype=torch.long).to(image.dev   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   \u001b[0mbs = image.size(\u001b[94m0\u001b[0m)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/LAVIS/lavis/models/\u001b[0m\u001b[1;33meva_vit.py\u001b[0m:\u001b[94m350\u001b[0m in \u001b[92mforward\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m347 \u001b[0m\u001b[2m#             return x[:, 0]\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m348 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m349 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x):                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m350 \u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.forward_features(x)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m351 \u001b[0m\u001b[2m#         x = self.head(x)\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m352 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m353 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/LAVIS/lavis/models/\u001b[0m\u001b[1;33meva_vit.py\u001b[0m:\u001b[94m339\u001b[0m in \u001b[92mforward_features\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.use_checkpoint:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m337 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mx = checkpoint.checkpoint(blk, x, rel_pos_bias)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m338 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m339 \u001b[2m│   │   │   │   \u001b[0mx = blk(x, rel_pos_bias)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m341 \u001b[0m\u001b[2m#         x = self.norm(x)\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m342 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/LAVIS/lavis/models/\u001b[0m\u001b[1;33meva_vit.py\u001b[0m:\u001b[94m175\u001b[0m in \u001b[92mforward\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x, rel_pos_bias=\u001b[94mNone\u001b[0m):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.gamma_1 \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m175 \u001b[2m│   │   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.drop_path(\u001b[96mself\u001b[0m.attn(\u001b[96mself\u001b[0m.norm1(x), rel_pos_bias=rel_pos_bias))    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.drop_path(\u001b[96mself\u001b[0m.mlp(\u001b[96mself\u001b[0m.norm2(x)))                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.drop_path(\u001b[96mself\u001b[0m.gamma_1 * \u001b[96mself\u001b[0m.attn(\u001b[96mself\u001b[0m.norm1(x), rel_pos_bias=   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/denny/LAVIS/lavis/models/\u001b[0m\u001b[1;33meva_vit.py\u001b[0m:\u001b[94m124\u001b[0m in \u001b[92mforward\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.q_bias \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   │   \u001b[0mqkv_bias = torch.cat((\u001b[96mself\u001b[0m.q_bias, torch.zeros_like(\u001b[96mself\u001b[0m.v_bias, requires_gr   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permut\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m124 \u001b[2m│   │   \u001b[0mqkv = F.linear(\u001b[96minput\u001b[0m=x, weight=\u001b[96mself\u001b[0m.qkv.weight, bias=qkv_bias)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0mqkv = qkv.reshape(B, N, \u001b[94m3\u001b[0m, \u001b[96mself\u001b[0m.num_heads, -\u001b[94m1\u001b[0m).permute(\u001b[94m2\u001b[0m, \u001b[94m0\u001b[0m, \u001b[94m3\u001b[0m, \u001b[94m1\u001b[0m, \u001b[94m4\u001b[0m)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0mq, k, v = qkv[\u001b[94m0\u001b[0m], qkv[\u001b[94m1\u001b[0m], qkv[\u001b[94m2\u001b[0m]   \u001b[2m# make torchscript happy (cannot use tensor a\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m18.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m23.70\u001b[0m GiB total capacity; \u001b[1;36m19.79\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m9.69\u001b[0m MiB free; \u001b[1;36m22.35\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory\n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SD2_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:51<00:00,  1.59it/s]\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 750/750 [09:30<00:00,  1.31it/s]\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:52<00:00,  1.59it/s]\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 750/750 [09:27<00:00,  1.32it/s]\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/denny/anaconda3/envs/instblip/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_IF_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:55<00:00,  1.58it/s]\n",
      "100%|██████████| 750/750 [09:18<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_DALLE_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:44<00:00,  1.62it/s]\n",
      "100%|██████████| 750/750 [09:27<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_SD2_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:42<00:00,  1.62it/s]\n",
      "100%|██████████| 750/750 [09:31<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_SDXL_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:45<00:00,  1.61it/s]\n",
      "100%|██████████| 750/750 [09:34<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_IF_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [07:48<00:00,  1.60it/s]\n",
      "100%|██████████| 750/750 [09:19<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_DALLE_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = q1\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)\n",
    "\n",
    "question = q2\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\", logPath=logPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [08:21<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "\n",
      "Acc: 70.57%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1756      | 1244       |\n",
      "Question: Is this photo real?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 70.57%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1756      | 1244       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 99.67%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 21.33%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 1180      | 320        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 61.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 576       | 924        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_IF_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = \"Is this photo real?\"\n",
    "# question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:25<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "Acc: 95.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 138       | 2862       |\n",
      "Question: Is this photo [*]?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 95.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 138       | 2862       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 95.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 93.07%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 104       | 1396       |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 97.73%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 34        | 1466       |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_IF_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "# question = \"Is this photo real?\"\n",
    "question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [08:32<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "\n",
      "Acc: 70.20%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2968      | 32         |\n",
      "GT false | 1756      | 1244       |\n",
      "Question: Is this photo real?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 70.20%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2968      | 32         |\n",
      "GT false | 1756      | 1244       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 98.93%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2968      | 32         |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 21.33%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 1180      | 320        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 61.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 576       | 924        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_IF_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = \"Is this photo real?\"\n",
    "# question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:30<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "Acc: 95.75%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2883      | 117        |\n",
      "GT false | 138       | 2862       |\n",
      "Question: Is this photo [*]?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 95.75%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2883      | 117        |\n",
      "GT false | 138       | 2862       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 96.10%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2883      | 117        |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 93.07%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 104       | 1396       |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 97.73%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 34        | 1466       |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_IF_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "# question = \"Is this photo real?\"\n",
    "question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [08:18<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "\n",
      "Acc: 67.77%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2968      | 32         |\n",
      "GT false | 1902      | 1098       |\n",
      "Question: Is this photo real?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 67.77%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2968      | 32         |\n",
      "GT false | 1902      | 1098       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 98.93%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2968      | 32         |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 23.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 1149      | 351        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 49.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 753       | 747        |\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = \"Is this photo real?\"\n",
    "# question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:20<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "Acc: 95.58%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2883      | 117        |\n",
      "GT false | 148       | 2852       |\n",
      "Question: Is this photo [*]?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 95.58%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2883      | 117        |\n",
      "GT false | 148       | 2852       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 96.10%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2883      | 117        |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 93.20%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 102       | 1398       |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 96.93%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 46        | 1454       |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "# question = \"Is this photo real?\"\n",
    "question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [08:07<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "\n",
      "Acc: 68.13%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1902      | 1098       |\n",
      "Question: Is this photo real?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 68.13%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1902      | 1098       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 99.67%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 23.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 1149      | 351        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 49.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 753       | 747        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_SDXL_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = \"Is this photo real?\"\n",
    "# question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:07<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "Acc: 95.23%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 148       | 2852       |\n",
      "Question: Is this photo [*]?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 95.23%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 148       | 2852       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 95.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 93.20%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 102       | 1398       |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 96.93%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 46        | 1454       |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_SDXL_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "# question = \"Is this photo real?\"\n",
    "question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [08:18<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo real?\n",
      "\n",
      "Acc: 79.92%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1195      | 1805       |\n",
      "Acc: 79.92%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1195      | 1805       |\n",
      "Question: Is this photo real?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 79.92%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 1195      | 1805       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 99.67%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2990      | 10         |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 43.07%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 854       | 646        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 77.27%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 341       | 1159       |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_SD2_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "question = \"Is this photo real?\"\n",
    "# question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:22<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "Acc: 96.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 66        | 2934       |\n",
      "Acc: 96.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 66        | 2934       |\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_label.csv\"\n",
    "# csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_COCO_SDXL_label.csv\"\n",
    "csv_path = \"/eva_data0/denny/textual_inversion/60k_6k_6k/test_Flickr_SD2_label.csv\"\n",
    "instruct.LoadData_batch(csv_path=csv_path)\n",
    "\n",
    "# question = \"Is this photo real?\"\n",
    "question = \"Is this photo [*]?\"\n",
    "acc, confusion_mat, ans_list, labels, label_3class = instruct.QueryImgs_batch(question=question, true_string=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 96.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 66        | 2934       |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 95.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 2862      | 138        |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 96.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 48        | 1452       |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 98.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 18        | 1482       |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "#image = Image.open(\"/eva_data0/denny/coco2014/val2014/COCO_val2014_000000000042.jpg\")\n",
    "image = Image.open(\"/eva_data0/denny/sd2/coco2014_train/samples/00000.png\")\n",
    "questions = [\"Is this photo real?\"]\n",
    "ans = instruct.Query(image, questions)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading imgs from /eva_data0/denny/coco2014/train2014: 100%|██████████| 200/200 [00:03<00:00, 51.07it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/SD2/0_real/: 100%|██████████| 100/100 [00:02<00:00, 33.49it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/SD2/1_fake/: 100%|██████████| 100/100 [00:03<00:00, 32.72it/s]\n"
     ]
    }
   ],
   "source": [
    "real_dir = \"/eva_data0/denny/coco2014/train2014\"\n",
    "fake_common_dir = \"/eva_data0/denny/SemanticError/SD2/0_real/\"\n",
    "fake_uncommon_dir = \"/eva_data0/denny/SemanticError/SD2/1_fake/\"\n",
    "real_num = 200\n",
    "num = [real_num, real_num//2, real_num//2]\n",
    "#num = [50, 25, 25]\n",
    "instruct.LoadData3Class(real_dir, fake_common_dir, fake_uncommon_dir, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 400/400 [02:17<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "Acc: 96.25%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 186       | 14         |\n",
      "GT false | 1         | 199        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc1, mat1, ans1 = instruct.QueryImgs(\"Is this photo [*]?\", true_string=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo [*]?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 96.25%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 186       | 14         |\n",
      "GT false | 1         | 199        |\n",
      "\n",
      "\n",
      "=== Real images ===\n",
      "Acc: 93.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 186       | 14         |\n",
      "GT false | 0         | 0          |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 99.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 1         | 99         |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 100.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 0         | 100        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [09:48<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "\n",
      "Acc: 79.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 819       | 181        |\n",
      "GT false | 231       | 769        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc1, mat1, ans1 = instruct.QueryImgs(\"Is this photo common in real world?\", true_string=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 79.40%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 819       | 181        |\n",
      "GT false | 231       | 769        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 64.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 180       | 320        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 89.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 51        | 449        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Real images ===\n",
      "Acc: 81.90%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 819       | 181        |\n",
      "GT false | 231       | 769        |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mat = np.array([[819,181],[231,769]])\n",
    "print(f'=== Real images ===')\n",
    "print(f'Acc: {(mat[0,0]/1000)*100:.2f}%')\n",
    "instruct.PrintConfusion(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [15:43<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by a model?\n",
      "\n",
      "Acc: 66.10%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 658       | 342        |\n",
      "GT false | 336       | 664        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo generated by a model?\"\n",
    "acc2, mat2, ans2 = instruct.QueryImgs(question, true_string=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by a model?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 66.10%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 658       | 342        |\n",
      "GT false | 336       | 664        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 71.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 141       | 359        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 61.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 195       | 305        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.85%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 560       | 440        |\n",
      "GT false | 103       | 897        |\n"
     ]
    }
   ],
   "source": [
    "acc, confusion_mat, ans = instruct.MultipleAns(ans1, ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by a model?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 72.85%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 560       | 440        |\n",
      "GT false | 103       | 897        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 85.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 71        | 429        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 93.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 32        | 468        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [10:16<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by AI?\n",
      "\n",
      "Acc: 50.75%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 998       | 2          |\n",
      "GT false | 983       | 17         |\n",
      "Question: Is this photo generated by AI?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 50.75%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 998       | 2          |\n",
      "GT false | 983       | 17         |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 2.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 486       | 14         |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 0.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 497       | 3          |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo generated by AI?\"\n",
    "instruct.QueryImgs(question, true_string=\"no\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by AI?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 50.75%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 998       | 2          |\n",
      "GT false | 983       | 17         |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 2.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 486       | 14         |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 0.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 497       | 3          |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 200/200 [01:25<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "\n",
      "Acc: 81.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 82        | 18         |\n",
      "GT false | 20        | 80         |\n",
      "Question: Is this photo common in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 81.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 82        | 18         |\n",
      "GT false | 20        | 80         |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 74.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 13        | 37         |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 86.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 7         | 43         |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo common in real world?\"\n",
    "acc1, mat1, ans1 = instruct.QueryImgs(question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 80.25%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 836       | 164        |\n",
      "GT false | 231       | 769        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 64.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 180       | 320        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 89.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 51        | 449        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [16:25<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo possible in real world?\n",
      "\n",
      "Acc: 57.45%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 740       | 260        |\n",
      "GT false | 591       | 409        |\n",
      "Question: Is this photo possible in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 57.45%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 740       | 260        |\n",
      "GT false | 591       | 409        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 39.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 301       | 199        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 42.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 290       | 210        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo possible in real world?\"\n",
    "instruct.QueryImgs(question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo possible in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 57.45%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 740       | 260        |\n",
      "GT false | 591       | 409        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 39.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 301       | 199        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 42.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 290       | 210        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading imgs from /eva_data0/denny/coco2014/train2014: 100%|██████████| 1000/1000 [00:03<00:00, 252.17it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/IF/0_real/: 100%|██████████| 500/500 [00:13<00:00, 35.73it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/IF/1_fake/: 100%|██████████| 500/500 [00:14<00:00, 33.47it/s]\n"
     ]
    }
   ],
   "source": [
    "real_dir = \"/eva_data0/denny/coco2014/train2014\"\n",
    "fake_common_dir = \"/eva_data0/denny/SemanticError/IF/0_real/\"\n",
    "fake_uncommon_dir = \"/eva_data0/denny/SemanticError/IF/1_fake/\"\n",
    "real_num = 1000\n",
    "num = [real_num, real_num//2, real_num//2]\n",
    "#num = [50, 25, 25]\n",
    "instruct.LoadData3Class(real_dir, fake_common_dir, fake_uncommon_dir, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [09:40<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "\n",
      "Acc: 74.95%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 836       | 164        |\n",
      "GT false | 337       | 663        |\n",
      "Question: Is this photo common in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 74.95%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 836       | 164        |\n",
      "GT false | 337       | 663        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 39.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 302       | 198        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 93.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 35        | 465        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo common in real world?\"\n",
    "acc1, mat1, ans1 = instruct.QueryImgs(question, true_string=\"yes\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 74.95%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 836       | 164        |\n",
      "GT false | 337       | 663        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 39.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 302       | 198        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 93.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 35        | 465        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [17:05<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by a model?\n",
      "\n",
      "Acc: 63.95%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 645       | 355        |\n",
      "GT false | 366       | 634        |\n",
      "Question: Is this photo generated by a model?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 63.95%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 645       | 355        |\n",
      "GT false | 366       | 634        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 63.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 185       | 315        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 63.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 181       | 319        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo generated by a model?\"\n",
    "acc2, mat2, ans2 = instruct.QueryImgs(question, true_string=\"no\")\n",
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by a model?\n",
      "\n",
      "=== Overall ===\n",
      "Acc: 63.95%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 645       | 355        |\n",
      "GT false | 366       | 634        |\n",
      "\n",
      "\n",
      "=== Common fake images ===\n",
      "Acc: 63.00%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 185       | 315        |\n",
      "\n",
      "\n",
      "=== Uncommon fake images ===\n",
      "Acc: 63.80%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 181       | 319        |\n"
     ]
    }
   ],
   "source": [
    "instruct.PrintResult(three_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.60%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 554       | 446        |\n",
      "GT false | 162       | 838        |\n"
     ]
    }
   ],
   "source": [
    "acc, confusion_mat, ans = instruct.MultipleAns(ans1, ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_t5_instruct\", model_type=\"flant5xl\", is_eval=True, device=device)\n",
    "#model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_t5_instruct\", model_type=\"flant5xxl\", is_eval=True, device=device)\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n",
    "#model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna13b\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading imgs from /eva_data0/denny/coco2014/train2014: 100%|██████████| 1000/1000 [00:03<00:00, 257.93it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/SD2/0_real/: 100%|██████████| 1000/1000 [00:22<00:00, 45.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# For query lots of images\n",
    "real_dir = \"/eva_data0/denny/coco2014/train2014\"\n",
    "fake_dir = \"/eva_data0/denny/SemanticError/SD2/0_real/\"\n",
    "#imgs, labels = PrepareData(\"/eva_data0/denny/SemanticError/SD2/test/\")\n",
    "imgs, labels = PrepareData(real_dir, fake_dir, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [17:37<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo possible in real world?\n",
      "Acc: 56.85%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 716       | 284        |\n",
      "GT false | 579       | 421        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For query lots of images\n",
    "question = \"Is this photo possible in real world?\"\n",
    "acc, confusion_mat, com_acc, com_conf_mat, uncom_acc, uncom_conf_mat = QueryImgs(imgs, labels, question, model, vis_processors, txt_processors, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "PrintConfusion(confusion_mat)\n",
    "\n",
    "print(com_acc)\n",
    "PrintConfusion(com_conf_mat)\n",
    "print(uncom_acc)\n",
    "PrintConfusion(uncom_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading imgs from /eva_data0/denny/coco2014/train2014: 100%|██████████| 200/200 [00:01<00:00, 199.41it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/SD2/1_fake/: 100%|██████████| 200/200 [00:03<00:00, 56.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# For query lots of images\n",
    "real_dir = \"/eva_data0/denny/coco2014/train2014\"\n",
    "fake_dir = \"/eva_data0/denny/SemanticError/SD2/1_fake/\"\n",
    "#imgs, labels = PrepareData(\"/eva_data0/denny/SemanticError/SD2/test/\")\n",
    "imgs, labels = PrepareData(real_dir, fake_dir, num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo possible in real world?\n",
      "Acc: 60.50%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 74        | 26         |\n",
      "GT false | 53        | 47         |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For query lots of images\n",
    "question = \"Is this photo possible in real world?\"\n",
    "acc, confusion_mat, com_acc, com_conf_mat, uncom_acc, uncom_conf_mat = QueryImgs(imgs, labels, question, model, vis_processors, txt_processors, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.605\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 74        | 26         |\n",
      "GT false | 53        | 47         |\n",
      "0.46\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 27        | 23         |\n",
      "0.46\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 27        | 23         |\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "PrintConfusion(confusion_mat)\n",
    "\n",
    "print(com_acc)\n",
    "PrintConfusion(com_conf_mat)\n",
    "print(uncom_acc)\n",
    "PrintConfusion(uncom_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [10:51<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo common in real world?\n",
      "Acc: 83.70%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 812       | 188        |\n",
      "GT false | 138       | 862        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For query lots of images\n",
    "question = \"Is this photo common in real world?\"\n",
    "acc, confusion_mat = QueryImgs(imgs, labels, question, model, vis_processors, txt_processors, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 2000/2000 [15:58<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this photo generated by a model?\n",
      "Acc: 62.55%\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 630       | 370        |\n",
      "GT false | 379       | 621        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo generated by a model?\"\n",
    "acc, confusion_mat = QueryImgs(imgs, labels, question, model, vis_processors, txt_processors, device, true_string=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading imgs from /eva_data0/denny/coco2014/train2014: 100%|██████████| 1000/1000 [00:04<00:00, 244.46it/s]\n",
      "Loading imgs from /eva_data0/denny/SemanticError/SD2/real_and_fake/: 100%|██████████| 1000/1000 [00:10<00:00, 91.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# For query lots of images\n",
    "real_dir = \"/eva_data0/denny/coco2014/train2014\"\n",
    "fake_dir = \"/eva_data0/denny/SemanticError/SD2/real_and_fake/\"\n",
    "#imgs, labels = PrepareData(\"/eva_data0/denny/SemanticError/SD2/test/\")\n",
    "imgs, labels = PrepareData(real_dir, fake_dir, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering:  84%|████████▎ | 1671/2000 [13:03<02:13,  2.46it/s]"
     ]
    }
   ],
   "source": [
    "question = \"Is this photo generated by a model?\"\n",
    "acc, confusion_mat, com_acc, com_conf_mat, uncom_acc, uncom_conf_mat = QueryImgs(imgs, labels, question, model, vis_processors, txt_processors, device, true_string=\"no\")\n",
    "print(acc)\n",
    "PrintConfusion(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 64        | 36         |\n",
      "GT false | 21        | 79         |\n",
      "0.78\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 11        | 39         |\n",
      "0.78\n",
      "         | Pred true | Pred false |\n",
      "GT true  | 0         | 0          |\n",
      "GT false | 11        | 39         |\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "PrintConfusion(confusion_mat)\n",
    "\n",
    "print(com_acc)\n",
    "PrintConfusion(com_conf_mat)\n",
    "print(uncom_acc)\n",
    "PrintConfusion(uncom_conf_mat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_image = Image.open(\"../docs/_static/merlion.png\").convert(\"RGB\")\n",
    "raw_image = Image.open(\"/eva_data/denny/SemanticError/SD2/1_fake/00000.png\").convert(\"RGB\")\n",
    "#display(raw_image.resize((596, 437)))\n",
    "display(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load instructBLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_t5_instruct\", model_type=\"flant5xl\", is_eval=True, device=device)\n",
    "#model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_t5_instruct\", model_type=\"flant5xxl\", is_eval=True, device=device)\n",
    "#model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n",
    "#model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna13b\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_processors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_processors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"Which city is this photo taken?\"\n",
    "#question = \"What is the animal in the photo?\"\n",
    "question = \"Is this photo possible in real world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"eval\" processors for inference\n",
    "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "question = txt_processors[\"eval\"](question)\n",
    "\n",
    "samples = {\"image\": image, \"text_input\": question}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generative question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_answers(samples=samples, inference_method=\"generate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ranking-based question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank answer candidates by their likelihood and return the best answer\n",
    "answer_candidates = [\"Singapore\", \"London\", \"Palo Alto\", \"Tokyo\"]\n",
    "\n",
    "model.predict_answers(samples, answer_list=answer_candidates, inference_method=\"rank\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask questions in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "# create a batch of samples, could be multiple images or copies of the same image\n",
    "image_batch = image.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "# create a batch of questions, make sure the number of questions matches the number of images\n",
    "question_1 = txt_processors[\"eval\"](\"Which city is this photo taken?\")\n",
    "question_2 = txt_processors[\"eval\"](\"What time is this during the day?\")\n",
    "question_3 = txt_processors[\"eval\"](\"Is it Singapore or London?\")\n",
    "\n",
    "question_batch = [question_1, question_2, question_3]\n",
    "\n",
    "model.predict_answers(samples={\"image\": image_batch, \"text_input\": question_batch}, inference_method=\"generate\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instblip",
   "language": "python",
   "name": "instblip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
